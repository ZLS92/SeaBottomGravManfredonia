# -*- coding: utf-8 -*-
"""
Created on Mon Sep 16 13:55:41 2019

@author: lzampa
"""

# -----------------------------------------------------------------------------
# Import libraries

from collections import namedtuple
from . import utils as utl
from . import plot_tools as lz_plot
# import lszpy.te_harmonica as te_hm


# -----------------------------------------------------------------------------
# Set the aliases for some libraries from the utils module
np = utl.np
os = utl.os
plt = utl.plt
copy = utl.copy
dt = utl.datetime
datetime = dt.datetime
timedelta = dt.timedelta

# -----------------------------------------------------------------------------
# Alias for the current directory (main dir.)
mdir = os.path.dirname(os.path.abspath(__file__))

# ==============================================================================
# ==============================================================================
# CONSTANTS

G = utl.G
M = utl.M
a_wgs84 = utl.a_wgs84
c_wgs84 = utl.c_wgs84
R_wgs84 = utl.R_wgs84
J2_wgs84 = utl.J2_wgs84
w_wgs84 = utl.w_wgs84

# ==============================================================================
# ==============================================================================
# DATA

# IGRF = 13th Generation International Geomagnetic Reference Field Schmidt semi-normalised spherical harmonic coefficients, degree n=1,13
# in units nanoTesla for IGRF and definitive DGRF main-field models (degree n=1,8 nanoTesla/year for secular variation (SV))
# c/s deg ord IGRF IGRF   IGRF   IGRF   IGRF   IGRF   IGRF   IGRF   IGRF   DGRF   DGRF   DGRF   DGRF   DGRF   DGRF   DGRF   DGRF   DGRF   DGRF   DGRF     DGRF      DGRF      DGRF      DGRF     IGRF      SV

SHigrf = np.array( [ 
    [   0,  0,  0, 1900.0, 1905.0, 1910.0, 1915.0, 1920.0, 1925.0, 1930.0, 1935.0, 1940.0, 1945.0, 1950.0, 1955.0, 1960.0, 1965.0, 1970.0, 1975.0, 1980.0, 1985.0, 1990.0, 1995.0,   2000.0,    2005.0,    2010.0,    2015.0,   2020.0,    2025 ],
    [ 'g',  1,  0, -31543, -31464, -31354, -31212, -31060, -30926, -30805, -30715, -30654, -30594, -30554, -30500, -30421, -30334, -30220, -30100, -29992, -29873, -29775, -29692, -29619.4, -29554.63, -29496.57, -29441.46, -29404.8,     5.7 ], 
    [ 'g',  1,  1,  -2298,  -2298,  -2297,  -2306,  -2317,  -2318,  -2316,  -2306,  -2292,  -2285,  -2250,  -2215,  -2169,  -2119,  -2068,  -2013,  -1956,  -1905,  -1848,  -1784,  -1728.2,  -1669.05,  -1586.42,  -1501.77,  -1450.9,     7.4 ], 
    [ 'h',  1,  1,   5922,   5909,   5898,   5875,   5845,   5817,   5808,   5812,   5821,   5810,   5815,   5820,   5791,   5776,   5737,   5675,   5604,   5500,   5406,   5306,   5186.1,   5077.99,   4944.26,   4795.99,   4652.5,   -25.9 ], 
    [ 'g',  2,  0,   -677,   -728,   -769,   -802,   -839,   -893,   -951,  -1018,  -1106,  -1244,  -1341,  -1440,  -1555,  -1662,  -1781,  -1902,  -1997,  -2072,  -2131,  -2200,  -2267.7,  -2337.24,  -2396.06,  -2445.88,  -2499.6,   -11.0 ], 
    [ 'g',  2,  1,   2905,   2928,   2948,   2956,   2959,   2969,   2980,   2984,   2981,   2990,   2998,   3003,   3002,   2997,   3000,   3010,   3027,   3044,   3059,   3070,   3068.4,   3047.69,   3026.34,   3012.20,   2982.0,    -7.0 ], 
    [ 'h',  2,  1,  -1061,  -1086,  -1128,  -1191,  -1259,  -1334,  -1424,  -1520,  -1614,  -1702,  -1810,  -1898,  -1967,  -2016,  -2047,  -2067,  -2129,  -2197,  -2279,  -2366,  -2481.6,  -2594.50,  -2708.54,  -2845.41,  -2991.6,   -30.2 ], 
    [ 'g',  2,  2,    924,   1041,   1176,   1309,   1407,   1471,   1517,   1550,   1566,   1578,   1576,   1581,   1590,   1594,   1611,   1632,   1663,   1687,   1686,   1681,   1670.9,   1657.76,   1668.17,   1676.35,   1677.0,    -2.1 ], 
    [ 'h',  2,  2,   1121,   1065,   1000,    917,    823,    728,    644,    586,    528,    477,    381,    291,    206,    114,     25,    -68,   -200,   -306,   -373,   -413,   -458.0,   -515.43,   -575.73,   -642.17,   -734.6,   -22.4 ], 
    [ 'g',  3,  0,   1022,   1037,   1058,   1084,   1111,   1140,   1172,   1206,   1240,   1282,   1297,   1302,   1302,   1297,   1287,   1276,   1281,   1296,   1314,   1335,   1339.6,   1336.30,   1339.85,   1350.33,   1363.2,     2.2 ], 
    [ 'g',  3,  1,  -1469,  -1494,  -1524,  -1559,  -1600,  -1645,  -1692,  -1740,  -1790,  -1834,  -1889,  -1944,  -1992,  -2038,  -2091,  -2144,  -2180,  -2208,  -2239,  -2267,  -2288.0,  -2305.83,  -2326.54,  -2352.26,  -2381.2,    -5.9 ], 
    [ 'h',  3,  1,   -330,   -357,   -389,   -421,   -445,   -462,   -480,   -494,   -499,   -499,   -476,   -462,   -414,   -404,   -366,   -333,   -336,   -310,   -284,   -262,   -227.6,   -198.86,   -160.40,   -115.29,    -82.1,     6.0 ], 
    [ 'g',  3,  2,   1256,   1239,   1223,   1212,   1205,   1202,   1205,   1215,   1232,   1255,   1274,   1288,   1289,   1292,   1278,   1260,   1251,   1247,   1248,   1249,   1252.1,   1246.39,   1232.10,   1225.85,   1236.2,     3.1 ], 
    [ 'h',  3,  2,      3,     34,     62,     84,    103,    119,    133,    146,    163,    186,    206,    216,    224,    240,    251,    262,    271,    284,    293,    302,    293.4,    269.72,    251.75,    245.04,    241.9,    -1.1 ], 
    [ 'g',  3,  3,    572,    635,    705,    778,    839,    881,    907,    918,    916,    913,    896,    882,    878,    856,    838,    830,    833,    829,    802,    759,    714.5,    672.51,    633.73,    581.69,    525.7,   -12.0 ], 
    [ 'h',  3,  3,    523,    480,    425,    360,    293,    229,    166,    101,     43,    -11,    -46,    -83,   -130,   -165,   -196,   -223,   -252,   -297,   -352,   -427,   -491.1,   -524.72,   -537.03,   -538.70,   -543.4,     0.5 ], 
    [ 'g',  4,  0,    876,    880,    884,    887,    889,    891,    896,    903,    914,    944,    954,    958,    957,    957,    952,    946,    938,    936,    939,    940,    932.3,    920.55,    912.66,    907.42,    903.0,    -1.2 ], 
    [ 'g',  4,  1,    628,    643,    660,    678,    695,    711,    727,    744,    762,    776,    792,    796,    800,    804,    800,    791,    782,    780,    780,    780,    786.8,    797.96,    808.97,    813.68,    809.5,    -1.6 ], 
    [ 'h',  4,  1,    195,    203,    211,    218,    220,    216,    205,    188,    169,    144,    136,    133,    135,    148,    167,    191,    212,    232,    247,    262,    272.6,    282.07,    286.48,    283.54,    281.9,    -0.1 ], 
    [ 'g',  4,  2,    660,    653,    644,    631,    616,    601,    584,    565,    550,    544,    528,    510,    504,    479,    461,    438,    398,    361,    325,    290,    250.0,    210.65,    166.58,    120.49,     86.3,    -5.9 ], 
    [ 'h',  4,  2,    -69,    -77,    -90,   -109,   -134,   -163,   -195,   -226,   -252,   -276,   -278,   -274,   -278,   -269,   -266,   -265,   -257,   -249,   -240,   -236,   -231.9,   -225.23,   -211.03,   -188.43,   -158.4,     6.5 ], 
    [ 'g',  4,  3,   -361,   -380,   -400,   -416,   -424,   -426,   -422,   -415,   -405,   -421,   -408,   -397,   -394,   -390,   -395,   -405,   -419,   -424,   -423,   -418,   -403.0,   -379.86,   -356.83,   -334.85,   -309.4,     5.2 ], 
    [ 'h',  4,  3,   -210,   -201,   -189,   -173,   -153,   -130,   -109,    -90,    -72,    -55,    -37,    -23,      3,     13,     26,     39,     53,     69,     84,     97,    119.8,    145.15,    164.46,    180.95,    199.7,     3.6 ], 
    [ 'g',  4,  4,    134,    146,    160,    178,    199,    217,    234,    249,    265,    304,    303,    290,    269,    252,    234,    216,    199,    170,    141,    122,    111.3,    100.00,     89.40,     70.38,     48.0,    -5.1 ], 
    [ 'h',  4,  4,    -75,    -65,    -55,    -51,    -57,    -70,    -90,   -114,   -141,   -178,   -210,   -230,   -255,   -269,   -279,   -288,   -297,   -297,   -299,   -306,   -303.8,   -305.36,   -309.72,   -329.23,   -349.7,    -5.0 ], 
    [ 'g',  5,  0,   -184,   -192,   -201,   -211,   -221,   -230,   -237,   -241,   -241,   -253,   -240,   -229,   -222,   -219,   -216,   -218,   -218,   -214,   -214,   -214,   -218.8,   -227.00,   -230.87,   -232.91,   -234.3,    -0.3 ], 
    [ 'g',  5,  1,    328,    328,    327,    327,    326,    326,    327,    329,    334,    346,    349,    360,    362,    358,    359,    356,    357,    355,    353,    352,    351.4,    354.41,    357.29,    360.14,    363.2,     0.5 ], 
    [ 'h',  5,  1,   -210,   -193,   -172,   -148,   -122,    -96,    -72,    -51,    -33,    -12,      3,     15,     16,     19,     26,     31,     46,     47,     46,     46,     43.8,     42.72,     44.58,     46.98,     47.7,     0.0 ], 
    [ 'g',  5,  2,    264,    259,    253,    245,    236,    226,    218,    211,    208,    194,    211,    230,    242,    254,    262,    264,    261,    253,    245,    235,    222.3,    208.95,    200.26,    192.35,    187.8,    -0.6 ], 
    [ 'h',  5,  2,     53,     56,     57,     58,     58,     58,     60,     64,     71,     95,    103,    110,    125,    128,    139,    148,    150,    150,    154,    165,    171.9,    180.25,    189.01,    196.98,    208.3,     2.5 ], 
    [ 'g',  5,  3,      5,     -1,     -9,    -16,    -23,    -28,    -32,    -33,    -33,    -20,    -20,    -23,    -26,    -31,    -42,    -59,    -74,    -93,   -109,   -118,   -130.4,   -136.54,   -141.05,   -140.94,   -140.7,     0.2 ], 
    [ 'h',  5,  3,    -33,    -32,    -33,    -34,    -38,    -44,    -53,    -64,    -75,    -67,    -87,    -98,   -117,   -126,   -139,   -152,   -151,   -154,   -153,   -143,   -133.1,   -123.45,   -118.06,   -119.14,   -121.2,    -0.6 ], 
    [ 'g',  5,  4,    -86,    -93,   -102,   -111,   -119,   -125,   -131,   -136,   -141,   -142,   -147,   -152,   -156,   -157,   -160,   -159,   -162,   -164,   -165,   -166,   -168.6,   -168.05,   -163.17,   -157.40,   -151.2,     1.3 ], 
    [ 'h',  5,  4,   -124,   -125,   -126,   -126,   -125,   -122,   -118,   -115,   -113,   -119,   -122,   -121,   -114,    -97,    -91,    -83,    -78,    -75,    -69,    -55,    -39.3,    -19.57,     -0.01,     15.98,     32.3,     3.0 ], 
    [ 'g',  5,  5,    -16,    -26,    -38,    -51,    -62,    -69,    -74,    -76,    -76,    -82,    -76,    -69,    -63,    -62,    -56,    -49,    -48,    -46,    -36,    -17,    -12.9,    -13.55,     -8.03,      4.30,     13.5,     0.9 ], 
    [ 'h',  5,  5,      3,     11,     21,     32,     43,     51,     58,     64,     69,     82,     80,     78,     81,     81,     83,     88,     92,     95,     97,    107,    106.3,    103.85,    101.04,    100.12,     98.9,     0.3 ], 
    [ 'g',  6,  0,     63,     62,     62,     61,     61,     61,     60,     59,     57,     59,     54,     47,     46,     45,     43,     45,     48,     53,     61,     68,     72.3,     73.60,     72.78,     69.55,     66.0,    -0.5 ], 
    [ 'g',  6,  1,     61,     60,     58,     57,     55,     54,     53,     53,     54,     57,     57,     57,     58,     61,     64,     66,     66,     65,     65,     67,     68.2,     69.56,     68.69,     67.57,     65.5,    -0.3 ], 
    [ 'h',  6,  1,     -9,     -7,     -5,     -2,      0,      3,      4,      4,      4,      6,     -1,     -9,    -10,    -11,    -12,    -13,    -15,    -16,    -16,    -17,    -17.4,    -20.33,    -20.90,    -20.61,    -19.1,     0.0 ], 
    [ 'g',  6,  2,    -11,    -11,    -11,    -10,    -10,     -9,     -9,     -8,     -7,      6,      4,      3,      1,      8,     15,     28,     42,     51,     59,     68,     74.2,     76.74,     75.92,     72.79,     72.9,     0.4 ], 
    [ 'h',  6,  2,     83,     86,     89,     93,     96,     99,    102,    104,    105,    100,     99,     96,     99,    100,    100,     99,     93,     88,     82,     72,     63.7,     54.75,     44.18,     33.30,     25.1,    -1.6 ], 
    [ 'g',  6,  3,   -217,   -221,   -224,   -228,   -233,   -238,   -242,   -246,   -249,   -246,   -247,   -247,   -237,   -228,   -212,   -198,   -192,   -185,   -178,   -170,   -160.9,   -151.34,   -141.40,   -129.85,   -121.5,     1.3 ], 
    [ 'h',  6,  3,      2,      4,      5,      8,     11,     14,     19,     25,     33,     16,     33,     48,     60,     68,     72,     75,     71,     69,     69,     67,     65.1,     63.63,     61.54,     58.74,     52.8,    -1.3 ], 
    [ 'g',  6,  4,    -58,    -57,    -54,    -51,    -46,    -40,    -32,    -25,    -18,    -25,    -16,     -8,     -1,      4,      2,      1,      4,      4,      3,     -1,     -5.9,    -14.58,    -22.83,    -28.93,    -36.2,    -1.4 ], 
    [ 'h',  6,  4,    -35,    -32,    -29,    -26,    -22,    -18,    -16,    -15,    -15,     -9,    -12,    -16,    -20,    -32,    -37,    -41,    -43,    -48,    -52,    -58,    -61.2,    -63.53,    -66.26,    -66.64,    -64.5,     0.8 ], 
    [ 'g',  6,  5,     59,     57,     54,     49,     44,     39,     32,     25,     18,     21,     12,      7,     -2,      1,      3,      6,     14,     16,     18,     19,     16.9,     14.58,     13.10,     13.14,     13.5,     0.0 ], 
    [ 'h',  6,  5,     36,     32,     28,     23,     18,     13,      8,      4,      0,    -16,    -12,    -12,    -11,     -8,     -6,     -4,     -2,     -1,      1,      1,      0.7,      0.24,      3.02,      7.35,      8.9,     0.0 ], 
    [ 'g',  6,  6,    -90,    -92,    -95,    -98,   -101,   -103,   -104,   -106,   -107,   -104,   -105,   -107,   -113,   -111,   -112,   -111,   -108,   -102,    -96,    -93,    -90.4,    -86.36,    -78.09,    -70.85,    -64.7,     0.9 ], 
    [ 'h',  6,  6,    -69,    -67,    -65,    -62,    -57,    -52,    -46,    -40,    -33,    -39,    -30,    -24,    -17,     -7,      1,     11,     17,     21,     24,     36,     43.8,     50.94,     55.40,     62.41,     68.1,     1.0 ], 
    [ 'g',  7,  0,     70,     70,     71,     72,     73,     73,     74,     74,     74,     70,     65,     65,     67,     75,     72,     71,     72,     74,     77,     77,     79.0,     79.88,     80.44,     81.29,     80.6,    -0.1 ], 
    [ 'g',  7,  1,    -55,    -54,    -54,    -54,    -54,    -54,    -54,    -53,    -53,    -40,    -55,    -56,    -56,    -57,    -57,    -56,    -59,    -62,    -64,    -72,    -74.0,    -74.46,    -75.00,    -75.99,    -76.7,    -0.2 ], 
    [ 'h',  7,  1,    -45,    -46,    -47,    -48,    -49,    -50,    -51,    -52,    -52,    -45,    -35,    -50,    -55,    -61,    -70,    -77,    -82,    -83,    -80,    -69,    -64.6,    -61.14,    -57.80,    -54.27,    -51.5,     0.6 ], 
    [ 'g',  7,  2,      0,      0,      1,      2,      2,      3,      4,      4,      4,      0,      2,      2,      5,      4,      1,      1,      2,      3,      2,      1,      0.0,     -1.65,     -4.55,     -6.79,     -8.2,     0.0 ], 
    [ 'h',  7,  2,    -13,    -14,    -14,    -14,    -14,    -14,    -15,    -17,    -18,    -18,    -17,    -24,    -28,    -27,    -27,    -26,    -27,    -27,    -26,    -25,    -24.2,    -22.57,    -21.20,    -19.53,    -16.9,     0.6 ], 
    [ 'g',  7,  3,     34,     33,     32,     31,     29,     27,     25,     23,     20,      0,      1,     10,     15,     13,     14,     16,     21,     24,     26,     28,     33.3,     38.73,     45.24,     51.82,     56.5,     0.7 ], 
    [ 'h',  7,  3,    -10,    -11,    -12,    -12,    -13,    -14,    -14,    -14,    -14,      2,      0,     -4,     -6,     -2,     -4,     -5,     -5,     -2,      0,      4,      6.2,      6.82,      6.54,      5.59,      2.2,    -0.8 ], 
    [ 'g',  7,  4,    -41,    -41,    -40,    -38,    -37,    -35,    -34,    -33,    -31,    -29,    -40,    -32,    -32,    -26,    -22,    -14,    -12,     -6,     -1,      5,      9.1,     12.30,     14.00,     15.07,     15.8,     0.1 ], 
    [ 'h',  7,  4,     -1,      0,      1,      2,      4,      5,      6,      7,      7,      6,     10,      8,      7,      6,      8,     10,     16,     20,     21,     24,     24.0,     25.35,     24.96,     24.45,     23.5,    -0.2 ], 
    [ 'g',  7,  5,    -21,    -20,    -19,    -18,    -16,    -14,    -12,    -11,     -9,    -10,     -7,    -11,     -7,     -6,     -2,      0,      1,      4,      5,      4,      6.9,      9.37,     10.46,      9.32,      6.4,    -0.5 ], 
    [ 'h',  7,  5,     28,     28,     28,     28,     28,     29,     29,     29,     29,     28,     36,     28,     23,     26,     23,     22,     18,     17,     17,     17,     14.8,     10.93,      7.03,      3.27,     -2.2,    -1.1 ], 
    [ 'g',  7,  6,     18,     18,     18,     19,     19,     19,     18,     18,     17,     15,      5,      9,     17,     13,     13,     12,     11,     10,      9,      8,      7.3,      5.42,      1.64,     -2.88,     -7.2,    -0.8 ], 
    [ 'h',  7,  6,    -12,    -12,    -13,    -15,    -16,    -17,    -18,    -19,    -20,    -17,    -18,    -20,    -18,    -23,    -23,    -23,    -23,    -23,    -23,    -24,    -25.4,    -26.32,    -27.61,    -27.50,    -27.2,     0.1 ], 
    [ 'g',  7,  7,      6,      6,      6,      6,      6,      6,      6,      6,      5,     29,     19,     18,      8,      1,     -2,     -5,     -2,      0,      0,     -2,     -1.2,      1.94,      4.92,      6.61,      9.8,     0.8 ], 
    [ 'h',  7,  7,    -22,    -22,    -22,    -22,    -22,    -21,    -20,    -19,    -19,    -22,    -16,    -18,    -17,    -12,    -11,    -12,    -10,     -7,     -4,     -6,     -5.8,     -4.64,     -3.28,     -2.32,     -1.8,     0.3 ], 
    [ 'g',  8,  0,     11,     11,     11,     11,     11,     11,     11,     11,     11,     13,     22,     11,     15,     13,     14,     14,     18,     21,     23,     25,     24.4,     24.80,     24.41,     23.98,     23.7,     0.0 ], 
    [ 'g',  8,  1,      8,      8,      8,      8,      7,      7,      7,      7,      7,      7,     15,      9,      6,      5,      6,      6,      6,      6,      5,      6,      6.6,      7.62,      8.21,      8.89,      9.7,     0.1 ], 
    [ 'h',  8,  1,      8,      8,      8,      8,      8,      8,      8,      8,      8,     12,      5,     10,     11,      7,      7,      6,      7,      8,     10,     11,     11.9,     11.20,     10.84,     10.04,      8.4,    -0.2 ], 
    [ 'g',  8,  2,     -4,     -4,     -4,     -4,     -3,     -3,     -3,     -3,     -3,     -8,     -4,     -6,     -4,     -4,     -2,     -1,      0,      0,     -1,     -6,     -9.2,    -11.73,    -14.50,    -16.78,    -17.6,    -0.1 ], 
    [ 'h',  8,  2,    -14,    -15,    -15,    -15,    -15,    -15,    -15,    -15,    -14,    -21,    -22,    -15,    -14,    -12,    -15,    -16,    -18,    -19,    -19,    -21,    -21.5,    -20.88,    -20.03,    -18.26,    -15.3,     0.6 ], 
    [ 'g',  8,  3,     -9,     -9,     -9,     -9,     -9,     -9,     -9,     -9,    -10,     -5,     -1,    -14,    -11,    -14,    -13,    -12,    -11,    -11,    -10,     -9,     -7.9,     -6.88,     -5.59,     -3.16,     -0.5,     0.4 ], 
    [ 'h',  8,  3,      7,      7,      6,      6,      6,      6,      5,      5,      5,    -12,      0,      5,      7,      9,      6,      4,      4,      5,      6,      8,      8.5,      9.83,     11.83,     13.18,     12.8,    -0.2 ], 
    [ 'g',  8,  4,      1,      1,      1,      2,      2,      2,      2,      1,      1,      9,     11,      6,      2,      0,     -3,     -8,     -7,     -9,    -12,    -14,    -16.6,    -18.11,    -19.34,    -20.56,    -21.1,    -0.1 ], 
    [ 'h',  8,  4,    -13,    -13,    -13,    -13,    -14,    -14,    -14,    -15,    -15,     -7,    -21,    -23,    -18,    -16,    -17,    -19,    -22,    -23,    -22,    -23,    -21.5,    -19.71,    -17.41,    -14.60,    -11.7,     0.5 ], 
    [ 'g',  8,  5,      2,      2,      2,      3,      4,      4,      5,      6,      6,      7,     15,     10,     10,      8,      5,      4,      4,      4,      3,      9,      9.1,     10.17,     11.61,     13.33,     15.3,     0.4 ], 
    [ 'h',  8,  5,      5,      5,      5,      5,      5,      5,      5,      5,      5,      2,     -8,      3,      4,      4,      6,      6,      9,     11,     12,     15,     15.5,     16.22,     16.71,     16.16,     14.9,    -0.3 ], 
    [ 'g',  8,  6,     -9,     -8,     -8,     -8,     -7,     -7,     -6,     -6,     -5,    -10,    -13,     -7,     -5,     -1,      0,      0,      3,      4,      4,      6,      7.0,      9.36,     10.85,     11.76,     13.7,     0.3 ], 
    [ 'h',  8,  6,     16,     16,     16,     16,     17,     17,     18,     18,     19,     18,     17,     23,     23,     24,     21,     18,     16,     14,     12,     11,      8.9,      7.61,      6.96,      5.69,      3.6,    -0.4 ], 
    [ 'g',  8,  7,      5,      5,      5,      6,      6,      7,      8,      8,      9,      7,      5,      6,     10,     11,     11,     10,      6,      4,      2,     -5,     -7.9,    -11.25,    -14.05,    -15.98,    -16.5,    -0.1 ], 
    [ 'h',  8,  7,     -5,     -5,     -5,     -5,     -5,     -5,     -5,     -5,     -5,      3,     -4,     -4,      1,     -3,     -6,    -10,    -13,    -15,    -16,    -16,    -14.9,    -12.76,    -10.74,     -9.10,     -6.9,     0.5 ], 
    [ 'g',  8,  8,      8,      8,      8,      8,      8,      8,      8,      7,      7,      2,     -1,      9,      8,      4,      3,      1,     -1,     -4,     -6,     -7,     -7.0,     -4.87,     -3.54,     -2.02,     -0.3,     0.4 ], 
    [ 'h',  8,  8,    -18,    -18,    -18,    -18,    -19,    -19,    -19,    -19,    -19,    -11,    -17,    -13,    -20,    -17,    -16,    -17,    -15,    -11,    -10,     -4,     -2.1,     -0.06,      1.64,      2.26,      2.8,     0.0 ], 
    [ 'g',  9,  0,      8,      8,      8,      8,      8,      8,      8,      8,      8,      5,      3,      4,      4,      8,      8,      7,      5,      5,      4,      4,      5.0,      5.58,      5.50,      5.33,      5.0,     0.0 ], 
    [ 'g',  9,  1,     10,     10,     10,     10,     10,     10,     10,     10,     10,    -21,     -7,      9,      6,     10,     10,     10,     10,     10,      9,      9,      9.4,      9.76,      9.45,      8.83,      8.4,     0.0 ], 
    [ 'h',  9,  1,    -20,    -20,    -20,    -20,    -20,    -20,    -20,    -20,    -21,    -27,    -24,    -11,    -18,    -22,    -21,    -21,    -21,    -21,    -20,    -20,    -19.7,    -20.11,    -20.54,    -21.77,    -23.4,     0.0 ], 
    [ 'g',  9,  2,      1,      1,      1,      1,      1,      1,      1,      1,      1,      1,     -1,     -4,      0,      2,      2,      2,      1,      1,      1,      3,      3.0,      3.58,      3.45,      3.02,      2.9,     0.0 ], 
    [ 'h',  9,  2,     14,     14,     14,     14,     14,     14,     14,     15,     15,     17,     19,     12,     12,     15,     16,     16,     16,     15,     15,     15,     13.4,     12.69,     11.51,     10.76,     11.0,     0.0 ], 
    [ 'g',  9,  3,    -11,    -11,    -11,    -11,    -11,    -11,    -12,    -12,    -12,    -11,    -25,     -5,     -9,    -13,    -12,    -12,    -12,    -12,    -12,    -10,     -8.4,     -6.94,     -5.27,     -3.22,     -1.5,     0.0 ], 
    [ 'h',  9,  3,      5,      5,      5,      5,      5,      5,      5,      5,      5,     29,     12,      7,      2,      7,      6,      7,      9,      9,     11,     12,     12.5,     12.67,     12.75,     11.74,      9.8,     0.0 ], 
    [ 'g',  9,  4,     12,     12,     12,     12,     12,     12,     12,     11,     11,      3,     10,      2,      1,     10,     10,     10,      9,      9,      9,      8,      6.3,      5.01,      3.13,      0.67,     -1.1,     0.0 ], 
    [ 'h',  9,  4,     -3,     -3,     -3,     -3,     -3,     -3,     -3,     -3,     -3,     -9,      2,      6,      0,     -4,     -4,     -4,     -5,     -6,     -7,     -6,     -6.2,     -6.72,     -7.14,     -6.74,     -5.1,     0.0 ], 
    [ 'g',  9,  5,      1,      1,      1,      1,      1,      1,      1,      1,      1,     16,      5,      4,      4,     -1,     -1,     -1,     -3,     -3,     -4,     -8,     -8.9,    -10.76,    -12.38,    -13.20,    -13.2,     0.0 ], 
    [ 'h',  9,  5,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -3,     -3,      4,      2,     -2,     -3,     -5,     -5,     -5,     -6,     -6,     -7,     -8,     -8.4,     -8.16,     -7.42,     -6.88,     -6.3,     0.0 ], 
    [ 'g',  9,  6,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -3,     -5,      1,     -1,     -1,      0,     -1,     -1,     -1,     -2,     -1,     -1.5,     -1.25,     -0.76,     -0.10,      1.1,     0.0 ], 
    [ 'h',  9,  6,      8,      8,      8,      8,      9,      9,      9,      9,      9,      9,      8,     10,      9,     10,     10,     10,      9,      9,      9,      8,      8.4,      8.10,      7.97,      7.79,      7.8,     0.0 ], 
    [ 'g',  9,  7,      2,      2,      2,      2,      2,      2,      3,      3,      3,     -4,     -2,      2,     -2,      5,      3,      4,      7,      7,      7,     10,      9.3,      8.76,      8.43,      8.68,      8.8,     0.0 ], 
    [ 'h',  9,  7,     10,     10,     10,     10,     10,     10,     10,     11,     11,      6,      8,      7,      8,     10,     11,     11,     10,      9,      8,      5,      3.8,      2.92,      2.14,      1.04,      0.4,     0.0 ], 
    [ 'g',  9,  8,     -1,      0,      0,      0,      0,      0,      0,      0,      1,     -3,      3,      2,      3,      1,      1,      1,      2,      1,      1,     -2,     -4.3,     -6.66,     -8.42,     -9.06,     -9.3,     0.0 ], 
    [ 'h',  9,  8,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,      1,    -11,     -6,      0,     -4,     -2,     -3,     -6,     -7,     -7,     -8,     -8.2,     -7.73,     -6.08,     -3.89,     -1.4,     0.0 ], 
    [ 'g',  9,  9,     -1,     -1,     -1,     -1,     -1,     -1,     -2,     -2,     -2,     -4,      8,      5,     -1,     -2,     -1,     -2,     -5,     -5,     -6,     -8,     -8.2,     -9.22,    -10.08,    -10.54,    -11.9,     0.0 ], 
    [ 'h',  9,  9,      2,      2,      2,      2,      2,      2,      2,      2,      2,      8,     -7,      5,      5,      1,      1,      1,      2,      2,      2,      3,      4.8,      6.01,      7.01,      8.44,      9.6,     0.0 ], 
    [ 'g', 10,  0,     -3,     -3,     -3,     -3,     -3,     -3,     -3,     -3,     -3,     -3,     -8,     -3,      1,     -2,     -3,     -3,     -4,     -4,     -3,     -3,     -2.6,     -2.17,     -1.94,     -2.01,     -1.9,     0.0 ], 
    [ 'g', 10,  1,     -4,     -4,     -4,     -4,     -4,     -4,     -4,     -4,     -4,     11,      4,     -5,     -3,     -3,     -3,     -3,     -4,     -4,     -4,     -6,     -6.0,     -6.12,     -6.24,     -6.26,     -6.2,     0.0 ], 
    [ 'h', 10,  1,      2,      2,      2,      2,      2,      2,      2,      2,      2,      5,     13,     -4,      4,      2,      1,      1,      1,      1,      2,      1,      1.7,      2.19,      2.73,      3.28,      3.4,     0.0 ], 
    [ 'g', 10,  2,      2,      2,      2,      2,      2,      2,      2,      2,      2,      1,     -1,     -1,      4,      2,      2,      2,      2,      3,      2,      2,      1.7,      1.42,      0.89,      0.17,     -0.1,     0.0 ], 
    [ 'h', 10,  2,      1,      1,      1,      1,      1,      1,      1,      1,      1,      1,     -2,      0,      1,      1,      1,      1,      0,      0,      1,      0,      0.0,      0.10,     -0.10,     -0.40,     -0.2,     0.0 ], 
    [ 'g', 10,  3,     -5,     -5,     -5,     -5,     -5,     -5,     -5,     -5,     -5,      2,     13,      2,      0,     -5,     -5,     -5,     -5,     -5,     -5,     -4,     -3.1,     -2.35,     -1.07,      0.55,      1.7,     0.0 ], 
    [ 'h', 10,  3,      2,      2,      2,      2,      2,      2,      2,      2,      2,    -20,    -10,     -8,      0,      2,      3,      3,      3,      3,      3,      4,      4.0,      4.46,      4.71,      4.55,      3.6,     0.0 ], 
    [ 'g', 10,  4,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -5,     -4,     -3,     -1,     -2,     -1,     -2,     -2,     -2,     -2,     -1,     -0.5,     -0.15,     -0.16,     -0.55,     -0.9,     0.0 ], 
    [ 'h', 10,  4,      6,      6,      6,      6,      6,      6,      6,      6,      6,     -1,      2,     -2,      2,      6,      4,      4,      6,      6,      6,      5,      4.9,      4.76,      4.44,      4.40,      4.8,     0.0 ], 
    [ 'g', 10,  5,      6,      6,      6,      6,      6,      6,      6,      6,      6,     -1,      4,      7,      4,      4,      6,      5,      5,      5,      4,      4,      3.7,      3.06,      2.45,      1.70,      0.7,     0.0 ], 
    [ 'h', 10,  5,     -4,     -4,     -4,     -4,     -4,     -4,     -4,     -4,     -4,     -6,     -3,     -4,     -5,     -4,     -4,     -4,     -4,     -4,     -4,     -5,     -5.9,     -6.58,     -7.22,     -7.92,     -8.6,     0.0 ], 
    [ 'g', 10,  6,      4,      4,      4,      4,      4,      4,      4,      4,      4,      8,     12,      4,      6,      4,      4,      4,      3,      3,      3,      2,      1.0,      0.29,     -0.33,     -0.67,     -0.9,     0.0 ], 
    [ 'h', 10,  6,      0,      0,      0,      0,      0,      0,      0,      0,      0,      6,      6,      1,      1,      0,      0,     -1,      0,      0,      0,     -1,     -1.2,     -1.01,     -0.96,     -0.61,     -0.1,     0.0 ], 
    [ 'g', 10,  7,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -1,      3,     -2,      1,      0,      1,      1,      1,      1,      1,      2,      2.0,      2.06,      2.13,      2.13,      1.9,     0.0 ], 
    [ 'h', 10,  7,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -1,     -1,     -4,     -3,     -3,     -1,     -2,     -1,     -1,     -1,     -1,     -2,     -2,     -2.9,     -3.47,     -3.95,     -4.16,     -4.3,     0.0 ], 
    [ 'g', 10,  8,      2,      2,      2,      1,      1,      1,      1,      2,      2,     -3,      2,      6,     -1,      2,      0,      0,      2,      2,      3,      5,      4.2,      3.77,      3.09,      2.33,      1.4,     0.0 ], 
    [ 'h', 10,  8,      4,      4,      4,      4,      4,      4,      4,      4,      4,     -2,      6,      7,      6,      3,      3,      3,      4,      4,      3,      1,      0.2,     -0.86,     -1.99,     -2.85,     -3.4,     0.0 ], 
    [ 'g', 10,  9,      2,      2,      2,      2,      3,      3,      3,      3,      3,      5,     10,     -2,      2,      2,      3,      3,      3,      3,      3,      1,      0.3,     -0.21,     -1.03,     -1.80,     -2.4,     0.0 ], 
    [ 'h', 10,  9,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     11,     -1,      0,      0,      1,      1,      0,      0,     -1,     -2,     -2.2,     -2.31,     -1.97,     -1.12,     -0.1,     0.0 ], 
    [ 'g', 10, 10,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -2,      3,      0,      0,      0,     -1,     -1,      0,      0,      0,      0,     -1.1,     -2.09,     -2.80,     -3.59,     -3.8,     0.0 ], 
    [ 'h', 10, 10,     -6,     -6,     -6,     -6,     -6,     -6,     -6,     -6,     -6,     -2,      8,     -3,     -7,     -6,     -4,     -5,     -6,     -6,     -6,     -7,     -7.4,     -7.93,     -8.31,     -8.72,     -8.8,     0.0 ], 
    [ 'g', 11,  0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      2.7,      2.95,      3.05,      3.00,      3.0,     0.0 ], 
    [ 'g', 11,  1,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -1.7,     -1.60,     -1.48,     -1.40,     -1.4,     0.0 ], 
    [ 'h', 11,  1,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.1,      0.26,      0.13,      0.00,      0.0,     0.0 ], 
    [ 'g', 11,  2,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -1.9,     -1.88,     -2.03,     -2.30,     -2.5,     0.0 ], 
    [ 'h', 11,  2,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      1.3,      1.44,      1.67,      2.11,      2.5,     0.0 ], 
    [ 'g', 11,  3,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      1.5,      1.44,      1.65,      2.08,      2.3,     0.0 ], 
    [ 'h', 11,  3,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.9,     -0.77,     -0.66,     -0.60,     -0.6,     0.0 ], 
    [ 'g', 11,  4,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.1,     -0.31,     -0.51,     -0.79,     -0.9,     0.0 ], 
    [ 'h', 11,  4,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -2.6,     -2.27,     -1.76,     -1.05,     -0.4,     0.0 ], 
    [ 'g', 11,  5,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.1,      0.29,      0.54,      0.58,      0.3,     0.0 ], 
    [ 'h', 11,  5,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.9,      0.90,      0.85,      0.76,      0.6,     0.0 ], 
    [ 'g', 11,  6,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.7,     -0.79,     -0.79,     -0.70,     -0.7,     0.0 ], 
    [ 'h', 11,  6,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.7,     -0.58,     -0.39,     -0.20,     -0.2,     0.0 ], 
    [ 'g', 11,  7,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.7,      0.53,      0.37,      0.14,     -0.1,     0.0 ], 
    [ 'h', 11,  7,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -2.8,     -2.69,     -2.51,     -2.12,     -1.7,     0.0 ], 
    [ 'g', 11,  8,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      1.7,      1.80,      1.79,      1.70,      1.4,     0.0 ], 
    [ 'h', 11,  8,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.9,     -1.08,     -1.27,     -1.44,     -1.6,     0.0 ], 
    [ 'g', 11,  9,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.1,      0.16,      0.12,     -0.22,     -0.6,     0.0 ], 
    [ 'h', 11,  9,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -1.2,     -1.58,     -2.11,     -2.57,     -3.0,     0.0 ], 
    [ 'g', 11, 10,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      1.2,      0.96,      0.75,      0.44,      0.2,     0.0 ], 
    [ 'h', 11, 10,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -1.9,     -1.90,     -1.94,     -2.01,     -2.0,     0.0 ], 
    [ 'g', 11, 11,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      4.0,      3.99,      3.75,      3.49,      3.1,     0.0 ], 
    [ 'h', 11, 11,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.9,     -1.39,     -1.86,     -2.34,     -2.6,     0.0 ], 
    [ 'g', 12,  0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -2.2,     -2.15,     -2.12,     -2.09,     -2.0,     0.0 ], 
    [ 'g', 12,  1,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.3,     -0.29,     -0.21,     -0.16,     -0.1,     0.0 ], 
    [ 'h', 12,  1,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.4,     -0.55,     -0.87,     -1.08,     -1.2,     0.0 ], 
    [ 'g', 12,  2,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.2,      0.21,      0.30,      0.46,      0.5,     0.0 ], 
    [ 'h', 12,  2,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.3,      0.23,      0.27,      0.37,      0.5,     0.0 ], 
    [ 'g', 12,  3,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.9,      0.89,      1.04,      1.23,      1.3,     0.0 ], 
    [ 'h', 12,  3,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      2.5,      2.38,      2.13,      1.75,      1.4,     0.0 ], 
    [ 'g', 12,  4,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.2,     -0.38,     -0.63,     -0.89,     -1.2,     0.0 ], 
    [ 'h', 12,  4,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -2.6,     -2.63,     -2.49,     -2.19,     -1.8,     0.0 ], 
    [ 'g', 12,  5,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.9,      0.96,      0.95,      0.85,      0.7,     0.0 ], 
    [ 'h', 12,  5,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.7,      0.61,      0.49,      0.27,      0.1,     0.0 ], 
    [ 'g', 12,  6,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.5,     -0.30,     -0.11,      0.10,      0.3,     0.0 ], 
    [ 'h', 12,  6,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.3,      0.40,      0.59,      0.72,      0.8,     0.0 ], 
    [ 'g', 12,  7,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.3,      0.46,      0.52,      0.54,      0.5,     0.0 ], 
    [ 'h', 12,  7,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.0,      0.01,      0.00,     -0.09,     -0.2,     0.0 ], 
    [ 'g', 12,  8,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.3,     -0.35,     -0.39,     -0.37,     -0.3,     0.0 ], 
    [ 'h', 12,  8,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.0,      0.02,      0.13,      0.29,      0.6,     0.0 ], 
    [ 'g', 12,  9,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.4,     -0.36,     -0.37,     -0.43,     -0.5,     0.0 ], 
    [ 'h', 12,  9,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.3,      0.28,      0.27,      0.23,      0.2,     0.0 ], 
    [ 'g', 12, 10,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.1,      0.08,      0.21,      0.22,      0.1,     0.0 ], 
    [ 'h', 12, 10,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.9,     -0.87,     -0.86,     -0.89,     -0.9,     0.0 ], 
    [ 'g', 12, 11,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.2,     -0.49,     -0.77,     -0.94,     -1.1,     0.0 ], 
    [ 'h', 12, 11,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.4,     -0.34,     -0.23,     -0.16,      0.0,     0.0 ], 
    [ 'g', 12, 12,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.4,     -0.08,      0.04,     -0.03,     -0.3,     0.0 ], 
    [ 'h', 12, 12,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.8,      0.88,      0.87,      0.72,      0.5,     0.0 ], 
    [ 'g', 13,  0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.2,     -0.16,     -0.09,     -0.02,      0.1,     0.0 ], 
    [ 'g', 13,  1,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.9,     -0.88,     -0.89,     -0.92,     -0.9,     0.0 ], 
    [ 'h', 13,  1,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.9,     -0.76,     -0.87,     -0.88,     -0.9,     0.0 ], 
    [ 'g', 13,  2,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.3,      0.30,      0.31,      0.42,      0.5,     0.0 ], 
    [ 'h', 13,  2,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.2,      0.33,      0.30,      0.49,      0.6,     0.0 ], 
    [ 'g', 13,  3,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.1,      0.28,      0.42,      0.63,      0.7,     0.0 ], 
    [ 'h', 13,  3,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      1.8,      1.72,      1.66,      1.56,      1.4,     0.0 ], 
    [ 'g', 13,  4,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.4,     -0.43,     -0.45,     -0.42,     -0.3,     0.0 ], 
    [ 'h', 13,  4,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.4,     -0.54,     -0.59,     -0.50,     -0.4,     0.0 ], 
    [ 'g', 13,  5,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      1.3,      1.18,      1.08,      0.96,      0.8,     0.0 ], 
    [ 'h', 13,  5,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -1.0,     -1.07,     -1.14,     -1.24,     -1.3,     0.0 ], 
    [ 'g', 13,  6,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.4,     -0.37,     -0.31,     -0.19,      0.0,     0.0 ], 
    [ 'h', 13,  6,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.1,     -0.04,     -0.07,     -0.10,     -0.1,     0.0 ], 
    [ 'g', 13,  7,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.7,      0.75,      0.78,      0.81,      0.8,     0.0 ], 
    [ 'h', 13,  7,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.7,      0.63,      0.54,      0.42,      0.3,     0.0 ], 
    [ 'g', 13,  8,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.4,     -0.26,     -0.18,     -0.13,      0.0,     0.0 ], 
    [ 'h', 13,  8,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.3,      0.21,      0.10,     -0.04,     -0.1,     0.0 ], 
    [ 'g', 13,  9,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.3,      0.35,      0.38,      0.38,      0.4,     0.0 ], 
    [ 'h', 13,  9,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.6,      0.53,      0.49,      0.48,      0.5,     0.0 ], 
    [ 'g', 13, 10,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.1,     -0.05,      0.02,      0.08,      0.1,     0.0 ], 
    [ 'h', 13, 10,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.3,      0.38,      0.44,      0.48,      0.5,     0.0 ], 
    [ 'g', 13, 11,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.4,      0.41,      0.42,      0.46,      0.5,     0.0 ], 
    [ 'h', 13, 11,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.2,     -0.22,     -0.25,     -0.30,     -0.4,     0.0 ], 
    [ 'g', 13, 12,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.0,     -0.10,     -0.26,     -0.35,     -0.5,     0.0 ], 
    [ 'h', 13, 12,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.5,     -0.57,     -0.53,     -0.43,     -0.4,     0.0 ], 
    [ 'g', 13, 13,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0.1,     -0.18,     -0.26,     -0.36,     -0.4,     0.0 ], 
    [ 'h', 13, 13,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,     -0.9,     -0.82,     -0.79,     -0.71,     -0.6,     0.0 ] 
    ] )

# ==============================================================================
# ==============================================================================
# FUNCTIONS

# -----------------------------------------------------------------------------
def g_shpere( dist=0, radius=0, density=None, mass=None ) :
    
    if mass is None :
        
        volume = ( 4/3 )  * np.pi * radius**3
        mass = density * volume
        
    g_sph = G * mass / ( ( dist + radius )**2 )

    return g_sph
    
# -----------------------------------------------------------------------------
def gn_67(lat):
    """
    Normal gravity calculated with the International Gravity Formula 1967 [mGal].
    Based on Hayford Elipsoid     
    """  
    
    a = 978031.846
    b = 0.005278895
    c = 0.000023462
    gn1 = b*np.power(np.sin(np.deg2rad(lat)),2)
    gn2 = +c*np.power(np.sin(np.deg2rad(lat)),4)
    gn3 = a*(1+gn1+gn2)
    
    return gn3

# -----------------------------------------------------------------------------
def gn_80( lat ):
    """
    Normal gravity calculated with the International Gravity Formula GRS80 [mGal]
    
    Ref:
    - Hinze et all., 2005 doi:10.1190/1.1988183
    """ 
          
    a = 978032.67715 # ge
    b = 0.001931851353 # k
    c = 0.0066943800229 # e^2  

    gna = 1 + b * np.power( np.sin( np.deg2rad ( lat ) ) , 2 ) # numerator
    gnb = np.sqrt( 1 - c * np.power( np.sin ( np.deg2rad ( lat ) ), 2 ) )  # denominator
    gnc = a * gna / gnb
    
    return gnc

# -----------------------------------------------------------------------------
def gn_84(lat):
    """
    Normal gravity calculated with WGS84 gravity formula [mGal]    
      
    Ref:
    - Decker, B. L. (1986). World geodetic system 1984
    """ 
    
    a = 978032.53359 
    b = 0.00193185138639
    c = 0.00669437999013
    lat = np.deg2rad(lat)
    gna = 1+b*np.power(np.sin(lat),2)
    gnb = np.sqrt(1-c*np.power(np.sin(lat),2))
    gnc = a*gna/gnb
    
    return gnc
    
# -----------------------------------------------------------------------------
def atm_c(h):
    """
    Atmospheric correction [mGal] 
    Ref:
    - Hinze et all., 2005 doi:10.1190/1.1988183
    """  

    a = 0.874
    b = 9.9*(10**(-5))
    c = 3.56*(10**(-9))
    gnb = b*h
    gnc = c*np.power(h,2)
    
    return a - gnb + gnc

# -----------------------------------------------------------------------------
def fa_c(h, lat=0, model='ell', R=R_wgs84):
    """
    Free Air Correction
    
    model=='sph': spherical approximation, with mean radius R [m]
    model=='ell': ellipsoidal approximation
    
    Ref:
    - Hinze et all., 2005 doi:10.1190/1.1988183
    """ 
    
    if model=='sph':
        fac = (-2*G*M/(R_wgs84**3))*h*1e5  

    if model=='ell':
        a = 0.3087691
        b = 0.0004398
        c = 7.2125*10**(-8) 
        fac = -(a-b*np.power(np.sin(np.deg2rad(lat)),2))*h+c*(np.power(h,2))
    
    return fac

# -----------------------------------------------------------------------------
def slab(h, dc=2670, dw=1030, topo_sea=False, st_type=None ):
    """
    Gravity effect of a flat slab [mGal]
    h = thickness    
    d = density    
    """
    
    if st_type is not None :
        ms = st_type == 1
        g_slb = 2 * np.pi * G * dc * h * 1e5
        g_slb[ms] = 2 * np.pi * G * ( -dw + dc ) * h[ms] * 1e5
    
    if ( topo_sea is True ) and ( st_type is None ) :
        ms = h < 0
        g_slb = 2 * np.pi * G * dc * h * 1e5
        g_slb[ms] = 2 * np.pi * G * ( -dw + dc ) * h[ms] * 1e5
        
    else:
        g_slb = 2 * np.pi * G * dc * h * 1e5
    
    return g_slb

# -----------------------------------------------------------------------------
def sph_shell(h, d):
    """
    Gravity effect of a spherical shell [mGal]
    h = thickness    
    d = density  
    """
    
    g_sphc = 4 * np.pi * G * d * h * 1e5
    
    return g_sphc

# -----------------------------------------------------------------------------
def curv_c(h=1, Rt=R_wgs84, Rd=167000, dc=2670, dw=1030, units=1e5, st_type=None):
    """
    Curvature correction for slab approx.
    
    h = station height [m]
    Rt = mean earth radius [m]
    Rd = slab radius [m]
    dc = crust density [kg/m3]
    dw = water density [kg/m3]
    units = if 1=[m/s2], if 1e5=[mGal]
    
    Ref:
    - Fullea et al., 2008 - FA2BOUGA FORTRAN 90 code to compute Bouguer gravity anomalies 
          from gridded free-air anomalies: Application to the Atlantic-Mediterranean transition zone;    
    - Whitman, 1991 - A microgal approximation for the Bullard Bearths curvaturegravity correction;    
    """
    
    if type(h) in (int, float) : 
        h=[h]
    alph = Rd/Rt
    cc = np.zeros(np.size(h))
    for i in range(0,np.size(h)):
        eta = h[i]/(Rt+h[i])
        
        if st_type == None :    
            if h[i] >= 0:  
                cc[i]=2*np.pi*G*dc*h[i]*(alph/2-eta/(2*alph)-eta)*units
            if h[i] < 0:  
                cc[i]=2*np.pi*G*(dc-dw)*h[i]*(alph/2+eta/(2*alph)+eta)*units 
                
        if st_type == 0 :
            cc[i]=2*np.pi*G*dc*h[i]*(alph/2-eta/(2*alph)-eta)*units 
            
        if st_type in ( 1, 2 ) :    
            cc[i]=2*np.pi*G*(dc-dw)*h[i]*(alph/2+eta/(2*alph)+eta)*units 
            
    return cc

# -----------------------------------------------------------------------------
def fw_c( h, 
          lat = 0, 
          g0 = None, 
          dw = 1030, 
          R = R_wgs84, 
          a0 = R_wgs84, 
          c0 = R_wgs84, 
          J2 = J2_wgs84, 
          w = w_wgs84, 
          model = 'sph' ):
    """
    Free Water Correction -- used for seaflor grav data [mGal] 
    
    Parameters:
        - h = station height [m] (NB. below s.l. h must be negative)
        - g0 = normal gravity 
        - lat = latitude [deg]
        - dw =  water density [kg/m3]
        - R = mean earth radius [m]
        - model='sph' --> dg/dh = -(2GM)/R + 4*pi*rho_w = spherical, 1st order taylor approx.
        - model='ell' --> ellipsoidal, 2nd order taylor approx.
    
    Returns:
        - fwc = free water correction [mGal]
    
    Ref: 
        - F. D. Stacey et al., 1981, doi: 10.1103/PhysRevD.23.1683
    """
    
    if g0==None:
        g0 = gn_80( lat ) * 1e-5 

    if model=='sph':
        fwc = ( - ( ( 2 * G * M ) / ( R**3 ) ) + 4 * np.pi * G * dw ) * h 

    if model=='ell':
        
        # geographic latitude in radians
        lat = np.deg2rad( lat )
        # geocentric latitude
        lat = np.arctan( ( ( c0**2 )/( a0**2 ) ) * np.tan( lat ) )
        
        f = ( ( a0 - c0 ) / a0 )
        r0 = a0*(1-((a0-c0)/a0)*(np.sin(lat)**2)) 
        r = r0+h
        a = r*(1+(((a0**2)/(c0**2))-1)*np.sin(lat)**2)
        c = a*(1-f)

        fwc1 = - 2 * ( g0/r0 ) * h * ( ( 1 +1.5* ( h/r0 ) ) + 3 * J2 * ( 1.5 * ( np.sin( lat )**2 )-0.5 ) )
        fwc2 = 3 * ( w**2 ) * h * ( 1-np.sin( lat )**2 )
        fwc3 = 4 * np.pi * G * ( ( ( c / a ) * ( 1 + 2*( h/r0 ) + 0.5 * ( ( a**2/c**2 )-1 ) * ( c**2 /a**2 ) ) ) \
                                * dw * h - ( dw * h**2 ) / r0 )

        fwc = fwc1 + fwc2 + fwc3

    fwc = fwc * 1e5
        
    return fwc

# -----------------------------------------------------------------------------
def ie( N, lat=0, dc=2670, dw=1030, xy=[], st_type=0, 
        method='nearest', prjcose_in=4326 ) :
    
    """
    Indarect effect -- used for disturbace correction [mGal] 
    
    N = Geoid height ( numpy array or list/tuple( x, y, N ) )
    den = density [kg/m3]
    xy = list or tuple with x and y coordinates of computational points
    method = interpolation method of geoid coordinates onto computational points x, y
    prjcode_in = proj code of input computational points coordinates (x, y)
    prjcode_out = proj code used for intepolating the geoid onto computational points
    
    Ref: 
    - Hinze et all., 2005 doi:10.1190/1.1988183
    """    
    
    if ( type(N) in (list, tuple) ) and ( len( N ) == 3 ) :
        xg, yg, N = N[0], N[1], N[2]  
        if utl.prj_(prjcose_in) != utl.prj_(4326) :
            lon, lat =utl.prjxy( prjcose_in, 4326, xg, yg )    
        
    if xy!=[] :
        x, y = xy
        N = utl.xyz2xy( ( xg, yg, N ), (x, y), method=method )
        
    if ( type( st_type ) in (int, float) ) and ( type( N ) not in (int, float) ) :
        st_type = np.full( N.shape, st_type )
    
    if type( N ) not in (int, float)  :    
        den = np.full( N.shape, dc )
        idx = st_type != 0 
        den[ idx ] = dw
    else :
        if st_type != 0 :
            den = dw
        else:
            den = dc    
    
    ie = - fa_c( N, lat=lat ) 
    dg_ie_bg = 2 * np.pi * G * den * N * 1e5
    geof_ie = ie - dg_ie_bg
    
    return [ ie, geof_ie, N ]

# -----------------------------------------------------------------------------
def line_filt( xyzl, wind_size=3, prjcode_in=4326, filter_type='median', 
               poly_order=6, prjcode_out=4326, x_c=0, y_c=1, z_c=2, line_c=3,
               extend_factor=2, deg2m=False, pad_mode='linear_ramp',
               new_xy=False, plot_lines=[], x_units='', y_units='[ mGal ]',
               edge_fix=True, gauss_sigma=2, order_c=None ) :

    # Create a copy of the input array
    xyzl = np.copy( xyzl )

    # If the input and output projection codes are different, 
    # convert the input coordinates to the output projection
    if prjcode_in != prjcode_out :
        xyzl[:,x_c], xyzl[:,y_c] = utl.prjxy( prjcode_in, prjcode_out, 
                                              xyzl[:,x_c], xyzl[:,y_c] )
    
    # Sort the input array by the line column and the order column
    xyzl = utl.sort_lines( xyzl, line_c=line_c, x_c=x_c, y_c=y_c, 
                           add_dist=False, order_c=order_c, add_original_ord=False )

    # Create a copy of the input array to store the output
    xyzl_new = np.copy( xyzl )

    # Create an array to store the lines numbers
    lines = np.unique( xyzl[ :, line_c ] )
        
    for i, l in enumerate( lines ) :
        
        idx = xyzl[ : , line_c ] == l

        line = xyzl[ idx ]
        line_new = np.copy( line )
        
        # Padding the line to avoid edge effects
        pad_val = np.pad( line_new[ : , z_c ], wind_size*extend_factor, 
                          mode=pad_mode, end_values=10 )

        if filter_type == 'uniform' : 
            filt_val = utl.sp.ndimage.uniform_filter( pad_val, wind_size, mode='nearest' )
            
        if filter_type == 'wiener' : 
            filt_val = utl.sp.signal.wiener( pad_val, mysize=wind_size )
            
        if filter_type == 'median' : 
            filt_val = utl.sp.signal.medfilt( pad_val, kernel_size=wind_size)
            
        if filter_type == 'savgol' : 
            filt_val = utl.sp.signal.savgol_filter( pad_val, wind_size, poly_order )  
            
        if filter_type == 'gauss' : 
            filt_val = utl.ndimage.gaussian_filter1d( pad_val, sigma=gauss_sigma )
            
        if edge_fix == True :

            diff = filt_val - pad_val
            w = np.zeros( diff.shape )
            w[0:wind_size*extend_factor+wind_size] = 1
            w[-(wind_size*extend_factor+wind_size):] = 1
            filt_w = utl.sp.ndimage.gaussian_filter1d( w, sigma=gauss_sigma ) 
            diff_w = diff * filt_w 
            filt_val = filt_val - diff_w
        
        line_new[ : , z_c ] = filt_val[ wind_size * extend_factor : 
                                        -wind_size * extend_factor ] 
            
        xyzl_new[ idx ] = line_new
        
    if plot_lines not in ( [], None, False ) :
        
        if plot_lines == True :
            plot_lines = []

        z_old_c = xyzl.shape[1]
        xyzl_new = np.column_stack( ( xyzl_new, xyzl[ :, z_c ] ) )
        fl = lz_plot.plot_lines( xyzl_new, line_c=line_c, x_c=x_c, y_c=y_c, z_c=[z_old_c,z_c], 
                              deg2m=deg2m, plot_points=False, marker='+', marker_color='k',
                              s=1.5, x_units=x_units, y_units=y_units, c=['b','g' ], 
                              legend=[ 'original_line', 'filtered_line' ], lines=plot_lines ) 
        
        xyzl_new = np.delete( xyzl_new, z_old_c, 1 )

    else : fl = None
        
    if ( new_xy is False ) and ( prjcode_in != prjcode_out )  :
        xyzl_new[:,x_c], xyzl_new[:,y_c] = utl.prjxy( prjcode_out, prjcode_in, 
                                                      xyzl_new[:,x_c], 
                                                      xyzl_new[:,y_c] )  
        
    return xyzl_new, fl
        
# -----------------------------------------------------------------------------
def line_remres( xyzl, 
                 xyz_ref, 
                 wind_size=None, 
                 prjcode_in=4326, 
                 prjcode_out=4326,
                 plot_lines=False, 
                 s=1, 
                 plot_cross=False, 
                 vminc=None, vmaxc=None,
                 pad_idx=-1, 
                 plot=False, 
                 vmin=None, vmax=None, 
                 new_xy=True,
                 x_c=0, y_c=1, z_c=2, 
                 line_c=3, 
                 units=None, 
                 ref_wl=16000,
                 wind_factor=1, 
                 xyz_w=None, 
                 x_units='', 
                 y_units='[ mGal ]',
                 adjst_lev=True, 
                 power=2, 
                 iterations=1, 
                 dist=None, 
                 spl_k=3, 
                 spl_s=0,
                 median_lev=False, 
                 radius=[], 
                 median_lines=[], 
                 order_c=None,
                 ref_res=1000, 
                 pad_dist=0, 
                 filt=None, 
                 wfilt=3, 
                 min_points=2 ) :

    xyzl = np.copy( xyzl )
    # xyz_ref = cop.copy( xyz_ref )
    line_c_origin = copy.copy( line_c )
    z_c_origin = copy.copy( z_c )
    x_c_origin = copy.copy( x_c )
    y_c_origin = copy.copy( y_c )
    order_c_origin = copy.copy( order_c )

    if type( xyz_ref ) in ( tuple, list ) :
       xyz_ref = np.column_stack( xyz_ref ) 
    #    utl.print_table( xyz_ref )

    if ( xyz_w is not None ) and ( type( xyz_w ) in ( tuple, list ) ) :
       xyz_w = np.column_stack( xyz_w ) 

    if prjcode_in != prjcode_out :
        xyzl[:,x_c], xyzl[:,y_c] = utl.prjxy( prjcode_in, prjcode_out, 
                                              xyzl[:,x_c], xyzl[:,y_c] )

        xyz_ref[:,0], xyz_ref[:,1] = utl.prjxy( prjcode_in, prjcode_out, 
                                            xyz_ref[:,0], xyz_ref[:,1] )
                                            

        if xyz_w is not None :
            xyz_w[:,0], xyz_w[:,1] = utl.prjxy( prjcode_in, prjcode_out, 
                                            xyz_w[:,0], xyz_w[:,1] ) 

    ref_gridstep = utl.min_dist( xyz_ref[:,0], xyz_ref[:,1] )['mean']
    print( 'ref_gridstep : ', ref_gridstep )

    xyzl_origin = np.copy( xyzl )

    # Getting the units of the input data (e.g. 'meters' or 'degree' )
    # based on the projection code of the output data
    if units == None :
        units = utl.prj_units( prjcode_out )

    # Getting the window size if not provided
    if wind_size is None :
        # Getting input reference field resolution
        ref_res = utl.min_dist( xyz_ref[0].ravel(), xyz_ref[1].ravel() )['mean']
        if wind_factor == 1:
            if units == 'degree' :
                wind_factor = int( ref_wl / utl.deg2m( ref_res ) )
            else :
                wind_factor = int( ref_wl / ref_res )
        wind_size = ref_res * wind_factor

    if units == 'degree' :
        print( 'window_size : ', round( utl.deg2m( wind_size ), 2 ) )
    else :   
        print( 'window_size : ', round( wind_size, 2 ) )
    
    half_ws =  wind_size / 2

    # Identify lines less then minimum number of points ( min_points )
    linu = np.unique( xyzl[:,line_c] )
    small_lines = []
    for l in linu :
        idx = xyzl[:,line_c] == l
        if np.sum( idx ) < min_points :
            small_lines.append( l )
        
    if dist is not None :

        xyzl = utl.resamp_lines( xyzl, dist, order_c=order_c_origin,
                                 line_c=line_c_origin, 
                                 x_c=x_c_origin, y_c=y_c_origin, 
                                 z_c=z_c_origin, plot=False )

        xyzl = np.column_stack( ( xyzl, np.arange( xyzl.shape[0] ) ) )
        x_c = 0
        y_c = 1 
        z_c = 2
        line_c = 3
        order_c = 4

    if pad_dist is None :
        pad_dist = wind_size

    if pad_dist != 0 :
        xyzl, idx_origin = utl.pad_lines( xyzl, pad_dist=pad_dist, pad_idx=pad_idx, 
            x_c=x_c, y_c=y_c, z_c=z_c, line_c=line_c, radius=half_ws, order_c=order_c, plot=False )
    
    i_no_pad = idx_origin >= 0

    lines = np.unique( xyzl_origin[:,line_c_origin] ) 

    print( 'Reference field interpolation ...' )
    
    ref_val = utl.xyz2xy( ( xyz_ref[:,0], xyz_ref[:,1], xyz_ref[:,2] ), 
                          ( xyzl[:,x_c], xyzl[:,y_c] ), method='cubic' )
    
    mean_ref_step = utl.min_dist( xyz_ref[:,0], xyz_ref[:,1] )['mean']

    print( 'Done!')
    
    if xyz_w is not None :

        print( 'Weights interpolation ...')
        
        weights  = utl.xyz2xy( (  xyz_w[:,0], xyz_w[:,1], xyz_w[:,2] ), 
                               ( xyzl[:,x_c], xyzl[:,y_c] ), 
                                 method='cubic', fillnan=True ) 
        
        print( 'Done!') 

    # Start RemRes loop
    # -------------------------------------------------------------------------
    print( 'RemRes loop ...')

    xyzl_out = np.copy( xyzl_origin )
    xyzl_out = np.column_stack( ( xyzl_origin, np.zeros( ( xyzl_origin.shape[0], 2 ) ) ) )
    xyzl_new = np.empty( ( xyzl.shape[0], xyzl.shape[1] + 4 ) )
    original_ref_val = np.zeros( xyzl_origin.shape[0] )

    for l in lines :

        idx = xyzl[ : , line_c ] == l
        idxo = xyzl_origin[ : , line_c_origin ] == l
        line = xyzl[ idx ]
        
        z_i = line[:,z_c]
        z_rem = np.zeros( line.shape[0] )
        z_ref = ref_val[idx]
        z_w = weights[idx]
        l_num = line[0, line_c]

        mean_line_step = utl.min_dist( line[:, x_c], line[:, y_c] )['mean']
        
        line_lim = utl.xy2lim( line[:,x_c], line[:,y_c], extend=half_ws )
        idx_ref = utl.xy_in_lim( xyz_ref[:,0], xyz_ref[:,1], line_lim )[2]
        xx_ref, yy_ref, zz_ref = utl.xyz2grid( x=xyz_ref[idx_ref,0], y=xyz_ref[idx_ref,1], z=xyz_ref[idx_ref,2],
                                               gstep=mean_ref_step, method='nearest' )[0]

        resamp_factor = int( mean_ref_step / mean_line_step )
        xx_ref, yy_ref, zz_ref = utl.resampling( ( xx_ref, yy_ref, zz_ref ), factor=resamp_factor  )       
        
        if ( line.shape[0] > 1 ) and ( l_num not in small_lines ) : 

            for i in range( line.shape[0] ) : 

                win_i = utl.neighboring_points( ( line[:,x_c], line[:,y_c] ), 
                                                ( line[i,x_c], line[i,y_c] ), half_ws )[1]
                
                win_ii = utl.neighboring_points( ( xx_ref, yy_ref ), 
                                                 ( line[i,x_c], line[i,y_c] ), half_ws )[1]

                pl_i = xyzl[idx, z_c][ win_i ]
                
                z_rem[i] = np.nanmean( pl_i ) 

                pl_ii = zz_ref[ win_ii ]
                
                z_ref[i] = np.nanmean( pl_ii )

            z_res = z_ref * z_w

            z_rem = z_rem * z_w

            z_new = z_i - z_rem + z_res 

        else :
            z_new = z_i
            
        xyzl_new[ idx, 0] = line[:,x_c]
        xyzl_new[ idx, 1] = line[:,y_c]
        xyzl_new[ idx, 2] = z_new
        xyzl_new[ idx, 3] = line[:,line_c]
        xyzl_new[ idx, 4] = z_rem
        xyzl_new[ idx, 5] = z_ref
        xyzl_new[ idx, 6] = z_i
        xyzl_new[ idx, 7] = ref_val[idx]

        dist_new = utl.geo_line_dist( xyzl_new[ idx & i_no_pad, 0], 
                                      xyzl_new[ idx & i_no_pad, 1] )

        dist_old = utl.geo_line_dist( xyzl_origin[ idxo, x_c_origin], 
                                      xyzl_origin[ idxo, y_c_origin] )

        z_new_no_pad = xyzl_new[ idx & i_no_pad, 2 ]
        z_ref_no_pad = xyzl_new[ idx & i_no_pad, 5 ]
        ref_val_no_pad = xyzl_new[ idx & i_no_pad, 7 ]

        xyzl_out[idxo, -1] = np.interp( dist_old[:,1], dist_new[:,1], 
                                        z_new_no_pad )
        xyzl_out[idxo, -2] = np.interp( dist_old[:,1], dist_new[:,1], 
                                        z_ref_no_pad )
        original_ref_val[idxo] = np.interp( dist_old[:,1], dist_new[:,1], 
                                            ref_val_no_pad )

    print( 'Done!')

    cover_ref = utl.cross_over_points( xyzl_new, x_c=0, y_c=1, z_c=5, line_c=3 )
    if cover_ref.shape[ 0 ] > 0 :
        print( '\nCross-over error of Ref-lines :')
        minz_ref, maxz_ref, meanz_ref, stdz_ref = utl.stat( cover_ref[ :, 6 ], decimals=2 )
    

    # Getting index of the columns 
    # of the corected data and 
    # of the reference data
    z_c_new = xyzl_out.shape[1]-1
    z_c_ref = xyzl_out.shape[1]-2

    # Calculate cross-over error

    # original c.o. error
    cover_o = utl.cross_over_points( xyzl_out, x_c=x_c_origin, y_c=y_c_origin, z_c=z_c_origin, 
                    line_c=line_c_origin, method='linear' )
    
    # new c.o. error after Remove-Restore
    cover_rr = utl.cross_over_points( xyzl_out, x_c=x_c_origin, y_c=y_c_origin, z_c=z_c_new, 
                    line_c=line_c_origin, method='linear' )
    
    if cover_o.shape[ 0 ] > 0 :
        print( '\nCross-over error before Remove-restore :')
        minz_o, maxz_o, meanz_o, stdz_o = utl.stat( cover_o[ :, 6 ], decimals=2 )
        print( '\nCross-over error after Remove-restore :')
        minz_rr, maxz_rr, meanz_rr, stdz_rr = utl.stat( cover_rr[ :, 6 ], decimals=2 )

    # Adjust levelling with LSQ method
    if adjst_lev == True : 
        print( '\nAdjust levelling ...') 
        xyzl_out, cover, _ = line_levellig( xyzl_out, x_c=x_c_origin, y_c=y_c_origin, z_c=z_c_new, 
                                          line_c=line_c_origin, power=power, iterations=iterations, 
                                          dist=dist, spl_k=spl_k, spl_s=spl_s, order_c=order_c_origin ) 
        print( '\n ... Done!') 

    # Adjust levelling with Median method
    if median_lev == True :
        print( '\nMedian levelling ...') 
        if median_lines == 'crossing' :
            cover = utl.cross_over_points( xyzl_out, x_c=x_c_origin, y_c=y_c_origin, z_c=z_c_new, 
                          line_c=line_c_origin, method='linear' )[:,3]    
            median_lines = np.unique( np.in1d( xyzl_out[:,line_c_origin], 
                                               np.unique( cover ), invert=True ) ).tolist()  
        if median_lines == [] :  
            median_lines = np.unique( xyzl_out[:,line_c_origin] ).tolist()

        xyzl_out, cover = median_levellig( xyzl_out, dist=dist, x_c=x_c_origin, y_c=y_c_origin,
                                    z_c=z_c_new, line_c=line_c_origin, radius=radius, lines=median_lines, 
                                    order_c=order_c_origin ) 
        print( '\n ...Done!') 

    #  Final filtering the corrected data (optional)
    if filt is not None :

        xyzl_out = line_filt( xyzl_out, wind_size=wfilt, prjcode_in=prjcode_in, 
                              filter_type=filt, poly_order=6, prjcode_out=prjcode_in, 
                              x_c=x_c_origin, y_c=y_c_origin, z_c=z_c_new, 
                              line_c=line_c_origin, extend_factor=2, edge_fix=True ) 
        
    # Calculate final c.o. error after all corrections
    cover_f = utl.cross_over_points( xyzl_out, x_c=x_c_origin, y_c=y_c_origin, z_c=z_c_new, 
                    line_c=line_c_origin, method='linear' )
    
    if cover_f.shape[ 0 ] > 0 : 
        print( '\nFinal cross-over error :')
        minz_f, maxz_f, meanz_f, stdz_f = utl.stat( cover_f[ :, 6 ], decimals=2 )

    # -------------------------------------------------------------------------
    # Plots
        
    if plot is True :
        
        plt.close("Data_Lev_Plot")
        plt.figure("Data_Lev_Plot", figsize=(9, 4))

        plt.subplot(1,2,1)

        vmin = np.nanmean( xyzl_origin[:,z_c_origin] ) - 2 * np.std( xyzl_origin[:,z_c_origin] )
        vmax = np.nanmean( xyzl_origin[:,z_c_origin] ) + 2 * np.std( xyzl_origin[:,z_c_origin] )

        ostat = utl.stat( xyzl_origin[:,z_c_origin], decimals=0, show=False )
        plt.title( 'Original : \n' + f'Min={ostat[0]}  Max={ostat[1]}  Mean={ostat[2]}  Std={ostat[3]}' )
        plt.scatter( xyzl_origin[:,x_c_origin], xyzl_origin[:,y_c_origin], 
                     s=s, c=xyzl_origin[:,z_c_origin], cmap='rainbow', vmin=vmin, vmax=vmax )
        plt.colorbar( )
        plt.gca().axes.xaxis.set_visible(False)
        plt.gca().axes.yaxis.set_visible(False)
        
        plt.subplot(1,2,2)

        lstat = utl.stat( xyzl_out[:,z_c_new], decimals=0, show=False )
        plt.title( 'Leveled : \n' + f'Min={lstat[0]}  Max={lstat[1]}  Mean={lstat[2]}  Std={lstat[3]}' )
        plt.scatter( xyzl_out[:,x_c_origin], xyzl_out[:,y_c_origin], s=s, c=xyzl_out[:,z_c_new], 
                     cmap='rainbow', vmin=vmin, vmax=vmax )
        plt.colorbar( )
        plt.gca().axes.xaxis.set_visible(False)
        plt.gca().axes.yaxis.set_visible(False)

        plt.tight_layout()
        plt.show()
        
    if plot_cross is True :

        if vminc == None :
            vminc = np.nanmin( cover_o[:,6] )
        if vmaxc == None :
            vmaxc = np.nanmax( cover_o[:,6] )
        
        plt.close("Data_Lev_CrossPlot")
        fig, axs = plt.subplots(1, 2, figsize=(8, 6), num="Data_Lev_CrossPlot")

        scatter1 = axs[0].scatter(cover_o[:, 0], cover_o[:, 1], s=s*10, c=cover_o[:, 6], cmap='rainbow',
                                vmin=vminc, vmax=vmaxc)
        axs[0].axes.xaxis.set_visible(False)
        axs[0].axes.yaxis.set_visible(False)
        axs[0].set_title('Cross-Over Error Original : \n' + f'Min={minz_o}  Max={maxz_o}  Mean={meanz_o}  Std={stdz_o}')

        scatter2 = axs[1].scatter(cover_f[:, 0], cover_f[:, 1], s=s*10, c=cover_f[:, 6], cmap='rainbow',
                                vmin=vminc, vmax=vmaxc)
        axs[1].axes.xaxis.set_visible(False)
        axs[1].axes.yaxis.set_visible(False)
        axs[1].set_title('Cross-Over Error Final : \n' + f' Min={minz_f}  Max={maxz_f}  Mean={meanz_f}  Std={stdz_f}')

        fig.tight_layout()

        cbar = fig.colorbar(scatter1, ax=axs, location='bottom', shrink=0.6)
        cbar.ax.text(1.02, 0.5, '[ mGal ]', va='center', ha='left', rotation=0, transform=cbar.ax.transAxes)

        plt.show()
    
    if plot_lines == True :
        
        if units == 'degree' : 
            deg2m = True
        else : 
            deg2m = False

        plt_line_array = np.column_stack( ( xyzl_out, original_ref_val ) )
        fl = lz_plot.plot_lines( plt_line_array, line_c=line_c_origin, x_c=x_c_origin, y_c=y_c_origin, 
                                 z_c=[z_c_origin, z_c_new, z_c_ref, -1], deg2m=deg2m, plot_points=False, 
                                 marker='+', marker_color='k', s=1.5, x_units=x_units, 
                                 y_units=y_units, c=['b','g','k', 'r'], 
                                 legend=[ 'original_line', 'leveled_line', 'reference_line', 'original_reference' ] ) 
        plt.show()
    else : 
        fl = None

    if new_xy is False :
        xyzl_out = np.copy( xyzl_out )
        xyzl_out[:,x_c_origin], xyzl_out[:,y_c_origin] = utl.prjxy( prjcode_out, prjcode_in, 
                                                                    xyzl_out[:,x_c_origin], 
                                                                    xyzl_out[:,y_c_origin] )
        
    print( z_c_new )
        
    return  xyzl_out, xyzl_new, cover_f, fl

# -----------------------------------------------------------------------------       
def line_levellig( xyzl, prjcode_in=4326, prjcode_out=4326, dist=None, x_c=0, y_c=1,
                   z_c=2, line_c=3, s=1, iterations=1, plot=False, vmin=None, vmax=None,
                   plot_cross=False, vminc=None, vmaxc=None, deg_to_m=False, power=2,
                   new_xy=True, x_units='', y_units='[mGal]', plot_lines=False,
                   spl_k=3, spl_s=None, order_c='same', lines=[] ) :
                   
    xyzl = np.copy( xyzl )
    if prjcode_in != prjcode_out :
        xyzl[:,x_c], xyzl[:,y_c] = utl.prjxy( prjcode_in, prjcode_out, 
                                              xyzl[:,x_c], xyzl[:,y_c] )
    
    if order_c != 'same' :
        xyzl = utl.sort_lines( xyzl, line_c=line_c, x_c=x_c, y_c=y_c, 
                               add_dist=False, order_c=order_c )

    if lines == [] :
        lines = np.unique( xyzl[ :, line_c ] )
        
    if dist == None :
        dist = utl.lines_samp_dist( xyzl, line_c=line_c, x_c=x_c, y_c=y_c, 
                                    deg_to_m=deg_to_m, kind='mode' ) 
        
    # Original c.o. error
    cover_o = utl.cross_over_points( xyzl, x_c=x_c, y_c=y_c, z_c=z_c, 
                    line_c=line_c, method='linear' )
    
    if cover_o.shape[ 0 ] > 0 :
        print( '\nCross-over error before leveling adjasments:')
        minz_o, maxz_o, meanz_o, stdz_o = utl.stat( cover_o[ :, 6 ], decimals=2 )

    xyzl_new = np.copy( xyzl )
    # -------------------------------------------------------------------------
    # Start itarations
    
    for itr in range( iterations ) :
        
        xyzl_re = utl.resamp_lines( xyzl_new, dist, order_c=order_c,
                                    line_c=line_c, x_c=x_c, y_c=y_c, z_c=z_c, lines=lines )

        cross_p = utl.cross_over_points( xyzl_re, method='linear', 
                                         x_c=0, y_c=1, z_c=2, line_c=3 )
        
        
        i1 = np.isin( cross_p[:,2], lines ) 
        i2 = np.isin( cross_p[:,3], lines )
        lines_cross = np.unique( np.concatenate( ( cross_p[i1,3], cross_p[i2,2], lines ) ) )
        cross_p = cross_p[ i1 | i2 ]
        
        N = np.size( lines_cross ) # Number of survey lines
        K = []
        W = []
        g = []
        
        for i, l in enumerate( lines_cross ) :
            
            if xyzl[ xyzl[ :, line_c ] == l ].shape[0] <= 1 : continue
            
            idx_c = ( cross_p[:,2] == l ) | ( cross_p[:,3] == l ) 
            cross_pi = cross_p[ idx_c ]
            Delta_g = np.zeros( cross_pi.shape[0] )
            
            if np.size( Delta_g ) == 0 :
                continue
            else :
                K.append( np.size( Delta_g ) ) 
                
                for li in range( K[-1] ) :
                    if cross_pi[li,2] == l :
                        g.append( cross_pi[li] ) 
                    if cross_pi[li,3] == l :  
                        col_change = [0,1,3,2,5,4,6]
                        g.append( cross_pi[ li, col_change ] ) 
                    
                    Delta_g[li] = g[-1][4] - g[-1][5]    
                
                if ( np.sum( Delta_g**2 ) == 0 ) or ( l not in lines ) :
                    w_line = [ K[-1] / 1e-50, l ] 
                else :    
                    w_line = [ K[-1] / np.sum( Delta_g**2 ), l ]
                W.append( w_line ) 
   
        g = np.array( g )
        W = np.array( W )        
        
        W_star = W[:,0] * ( N / np.sum( W[:,0] ) ) 
        
        g_ij = np.zeros( g.shape[0] )
        C_ij = np.zeros( g.shape[0] )
        
        for n, _ in enumerate( g ) :
            
            W_star_i = W_star[ W[:,1] == g[n,2] ] ** power
            W_star_j = W_star[ W[:,1] == g[n,3] ] ** power
                
#            m = ( W_star_j / W_star_i ) * power
#            g_ij[ n ] =  ( g[n,4] + g[n,5] * m ) / ( 1 + m )
            
            g_ij[ n ] = ( g[n,4] * W_star_i + g[n,5] * W_star_j ) / ( W_star_i + W_star_j )         
            C_ij[ n ] = g_ij[ n ] - g[n,4]
                    
        for l in W[:,1] :
            
            if l in lines :
            
                idx = xyzl_re[:, 3] == l 
                idx_cross = g[:,2] == l
                g_cross = C_ij[ idx_cross ]
                
                line_dists = np.linspace( 0, dist*np.size( xyzl_re[idx, 2] ), 
                                          np.size( xyzl_re[idx, 2] ) )            
                
                if ( np.size( g_cross ) == 1 ) : 
                    g_cross_line = np.repeat( g_cross[0], np.sum( idx ) )
                    
                if ( np.size( g_cross ) >= 2 ) :
                    x_start, y_start = xyzl_re[idx, 0][0], xyzl_re[idx, 1][0]  
                    
                    dist_cross =  np.sqrt( ( g[ idx_cross, 0 ] - x_start )**2 + 
                                           ( g[ idx_cross, 1 ] - y_start )**2 )

                    si = np.argsort( dist_cross )
                    dist_cross = dist_cross[si]
                    g_cross = g_cross[si]
                    int_c_f = utl.sp.interpolate.interp1d( dist_cross, g_cross, kind='nearest',
                              bounds_error=False, fill_value='extrapolate' ) 
                                                      
                    g_cross_line = int_c_f( line_dists )
                    
                    # Spline interpolation
                    # -------------------------------------------------------------
                    if g_cross_line.size <= spl_k :
                        spl_k2 = g_cross_line.size - 1

                    else : 
                        spl_k2 = spl_k

                    spl = utl.sp.interpolate.UnivariateSpline( line_dists, 
                                                               g_cross_line, 
                                                               k=spl_k2, 
                                                               s=spl_s ) 
                    g_cross_line = spl( line_dists )  

                xyzl_re[idx, 2] = xyzl_re[idx, 2] + g_cross_line
            
        for i, l in enumerate( lines ) :   

            idx_new = xyzl_new[ :, line_c ] == l
            line_new = xyzl_new[ xyzl_new[ :, line_c ] == l ]
            
            if line_new.shape[0] <= 1 : continue
            
            line_re = xyzl_re[ xyzl_re[ :, 3 ] == l ]
            
            dist_re = utl.geo_line_dist( line_re[:,0], line_re[:,1] )[:,1]  
            dist_new = utl.geo_line_dist( line_new[:,x_c], line_new[:, y_c] )[:,1] 
            
            if dist_re.size > 1 :
                int_new = utl.sp.interpolate.interp1d( dist_re, line_re[:, 2], 
                          kind='linear', bounds_error=False, fill_value='extrapolate' )   
                xyzl_new[idx_new, z_c] = int_new( dist_new )
            else :
              xyzl_new[idx_new, z_c] = line_re[:, 2] 
            
        
        # Original c.o. error
        cover_i = utl.cross_over_points( xyzl_new, vmin=vminc, vmax=vmaxc, method='linear',
                                        x_c=x_c, y_c=y_c, z_c=z_c, line_c=line_c ) 
        
        if cover_i.shape[ 0 ] > 0 :
            print( f'\nCross-over error after leveling adjasments, iteration { itr+1}:')
            minz_i, maxz_i, meanz_i, stdz_i = utl.stat( cover_i[ :, 6 ], decimals=2 )
    
    # -------------------------------------------------------------------------
    #Plots
    if plot is True :
        
        plt.figure()
        
        if vmin == None :
            vmin = np.nanmean( xyzl_new[:,z_c] ) - 2 * np.std( xyzl_new[:,z_c] )
        if vmax == None :
            vmax = np.nanmean( xyzl_new[:,z_c] ) + 2 * np.std( xyzl_new[:,z_c] )          
        
        plt.subplot(1,2,1)
        plt.title('Original')
        plt.scatter( xyzl[:,x_c], xyzl[:,y_c], s=s, c=xyzl[:,z_c], cmap='rainbow',
                     vmin=vmin, vmax=vmax )
        
        plt.subplot(1,2,2)
        plt.title('Leveled')
        plt.scatter( xyzl_new[:,x_c], xyzl_new[:,y_c], s=s, c=xyzl_new[:,z_c], cmap='rainbow',
                     vmin=vmin, vmax=vmax )
        
        plt.tight_layout()
        plt.colorbar( ax=plt.gcf().axes, location='bottom', shrink=0.6 )    
    
    if plot_cross is True :
        
        if vminc == None :
            vminc = np.nanmin( cover_o[:,6] )
        if vmaxc == None :
            vmaxc = np.nanmax( cover_o[:,6] )
        
        plt.figure(figsize=(8, 6))
        
        plt.subplot(1,2,1)
        plt.scatter( cover_o[:,0], cover_o[:,1], s=s*10, c=cover_o[:,6], cmap='rainbow',
                     vmin=vminc, vmax=vmaxc )
        plt.gca().axes.xaxis.set_visible(False)
        plt.gca().axes.yaxis.set_visible(False)
        
        plt.title( 'Cross-Over Error Original : \n' + f'Min={minz_o}  Max={maxz_o}  Mean={meanz_o}  Std={stdz_o}' )        

        plt.subplot(1,2,2)
        plt.scatter( cover_i[:,0], cover_i[:,1], s=s*10, c=cover_i[:,6], cmap='rainbow',
                      vmin=vminc, vmax=vmaxc )
        plt.gca().axes.xaxis.set_visible(False)
        plt.gca().axes.yaxis.set_visible(False)
        
        plt.title( 'Cross-Over Error Final : \n' + f' Min={minz_i}  Max={maxz_i}  Mean={meanz_i}  Std={stdz_i}' )    

        plt.tight_layout()

        cbar = plt.colorbar( ax=plt.gcf().axes, location='bottom', shrink=0.6 )     
        plt.text(1.02, 0.5, '[ mGal ]', va='center', ha='left', rotation=0, transform=cbar.ax.transAxes)
        
    if plot_lines == True :
        
        if deg_to_m == True : 
            deg2m = True
        else : 
            deg2m = False
        
        z_old_c = xyzl.shape[1]
        xyzl_new = np.column_stack( ( xyzl_new, xyzl[ :, z_c ] ) )
        fl = lz_plot.plot_lines( xyzl_new, line_c=line_c, x_c=x_c, y_c=y_c, z_c=[z_old_c,z_c], 
                              deg2m=deg2m, plot_points=False, marker='+', marker_color='k',
                              s=1.5, x_units=x_units, y_units=y_units, c=['b','g' ], 
                              legend=[ 'original_line', 'leveled_line' ] )  
        
        xyzl_new = np.delete( xyzl_new, z_old_c, 1 )
        
    else : 
        fl = None
    
    if new_xy is False :
        xyzl_new = np.copy( xyzl_new )
        xyzl_new[:,x_c], xyzl_new[:,y_c] = utl.prjxy( prjcode_out, prjcode_in, 
                                                      xyzl_new[:,x_c], 
                                                      xyzl_new[:,y_c] )         
    return xyzl_new, cover_i, fl
        
        
# -----------------------------------------------------------------------------       
def median_levellig( xyzl, prjcode_in=4326, prjcode_out=4326, dist=None, x_c=0, y_c=1,
                     z_c=2, line_c=3, s=1, iter=1, plot=False, vmin=None, vmax=None,
                     new_xy=True, plot_lines=False,radius=None, lines=[], 
                     deg_to_m=False, order_c='same' ) :  
                     
    
    xyzl = np.copy( xyzl )    
    if prjcode_in != prjcode_out :
        xyzl[:,x_c], xyzl[:,y_c] = utl.prjxy( prjcode_in, prjcode_out, 
                                              xyzl[:,x_c], xyzl[:,y_c] )
    
    if order_c != 'same' :
        xyzl = utl.sort_lines( xyzl, line_c=line_c, x_c=x_c, y_c=y_c, 
                               add_dist=False, order_c=order_c )
    
    if dist == None :
        dist = utl.lines_samp_dist( xyzl, line_c=line_c, x_c=x_c, y_c=y_c, 
                                    deg_to_m=deg_to_m, kind='mode' ) 
    
    xyzl_new = np.copy( xyzl )
    
    # -------------------------------------------------------------------------
    # Start itarations
    if radius is None :
        radius = [ dist*12, dist*6, dist*3 ]
    if type( radius ) in ( int, float ) :
        radius = [ radius for k in range( iter ) ]
    
    for itr in range( len(radius) ) :
        
        xyzl_re = utl.resamp_lines( xyzl, dist, order_c=order_c,
                                    line_c=line_c, x_c=x_c, y_c=y_c, z_c=z_c ) 
        
        if lines is [] :
            lines = np.unique( xyzl_re[ :, 3 ] )
        
        for i, l in enumerate( lines ) :
            
            line_new = xyzl_new[ xyzl_new[ :, line_c ] == l ]
            
            if line_new.shape[0] < 1 : continue
            
            lim_lin = utl.xy2lim( line_new[:,x_c], line_new[:,y_c], extend=True, 
                                  d=radius[itr]*2 )        
            xl_re, yl_re, idx_re = utl.xy_in_lim( xyzl_re[:,0], xyzl_re[:,1], lim_lin )
            zl_re = xyzl_re[ idx_re, 2 ] 
            line_re = xyzl_re[ idx_re, 3 ]            

            for ip, p in enumerate( line_new ) :
                
#                win_i = ( xyzl_re[:,0] > line_new[ip,x_c] - radius[itr] ) & \
#                        ( xyzl_re[:,0] < line_new[ip,x_c] + radius[itr] ) & \
#                        ( xyzl_re[:,1] > line_new[ip,y_c] - radius[itr] ) & \
#                        ( xyzl_re[:,1] < line_new[ip,y_c] + radius[itr] ) 
                        
                win_i = utl.neighbors_points( ( xl_re, yl_re ), 
                        ( line_new[ip,x_c], line_new[ip,y_c] ), radius[itr] )[2]
                        
                Am = np.nanmedian( zl_re[ win_i ] )
                Lm = np.median( zl_re[ win_i ][ line_re[win_i] == l] )
                line_new[ip, z_c] = line_new[ip, z_c] + Am - Lm
        
            xyzl_new[ xyzl_new[:,line_c] == l ] = line_new
                
    #Plots

        cover = utl.cross_over_points( xyzl_new, x_c=x_c, y_c=y_c, z_c=z_c, 
                    line_c=line_c, method='linear' )

        if cover.shape[0] > 0 :
            minz, maxz, meanz, stdz = utl.stat( cover[ :, 6], decimals=2 )

            print( 'iteration ', itr+1, ':\n', 
                    'cross-over error:', 'min =',minz,',', 'max =',maxz,',', 
                                        'mean =',meanz,',', 'std =',stdz )    
    
    if plot is True :
        plot_xyzl_o = xyzl[ np.in1d( xyzl[:,line_c], lines ) ]
        plot_xyzl_n = xyzl_new[ np.in1d( xyzl_new[:,line_c], lines ) ]        
        plt.figure()
        
        if vmin == None :
            vmin = np.nanmean( plot_xyzl_o[:,z_c] ) - 2 * np.std( plot_xyzl_o[:,z_c] )
        if vmax == None :
            vmax = np.nanmean( plot_xyzl_o[:,z_c] ) + 2 * np.std( plot_xyzl_o[:,z_c] )          
        
        plt.subplot(1,3,1)
        plt.title('Original')
        plt.scatter( plot_xyzl_o[:,x_c], plot_xyzl_o[:,y_c], s=s, c=plot_xyzl_o[:,z_c], cmap='rainbow',
                     vmin=vmin, vmax=vmax )
        
        plt.subplot(1,3,2)
        plt.title('Leveled')
        plt.scatter( plot_xyzl_n[:,x_c], plot_xyzl_n[:,y_c], s=s, c=plot_xyzl_n[:,z_c], cmap='rainbow',
                     vmin=vmin, vmax=vmax )
        plt.colorbar()
        
        plt.subplot(1,3,3)
        plt.title('Difference')
        plt.scatter( plot_xyzl_n[:,x_c], plot_xyzl_n[:,y_c], s=s, c=plot_xyzl_o[:,z_c]-plot_xyzl_n[:,z_c], 
                    cmap='rainbow' )  
        plt.colorbar()
                           
        
        plt.tight_layout()
#        plt.colorbar( ax=plt.gcf().axes, location='bottom', shrink=0.6 )    
    
    if new_xy is False :
        xyzl_new[:,x_c], xyzl_new[:,y_c] = utl.prjxy( prjcode_out, prjcode_in, 
                                                      xyzl_new[:,x_c], 
                                                      xyzl_new[:,y_c] )         
        
    return xyzl_new, cover  

# -----------------------------------------------------------------------------
def assign_loop_ids(
    stations, datetime=None, gobs=None,
    exclude_stations=[], closing_stations=[],
    DateTimeFormat='datetime64[s]', verbose=False, time_th=7200 ):
    
    """
    Assegna un LoopID a ciascuna misura in base alla sequenza temporale.

    Regole:
      - Se closing_stations  None -> chiusura quando si rivede una stazione gi vista.
      - Se closing_stations  una lista/insieme -> chiusura SOLO quando si incontra
        una stazione in closing_stations.
      - La misura di chiusura appartiene al loop corrente; il LOOP successivo inizia
        con la misura successiva.

    Parametri
    ---------
    stations : array-like
    datetime : array-like (str/np.datetime64 o secondi)
    gobs : array-like, opzionale (usato solo per stampa)
    exclude_stations : list, opzionale (LoopID=NaN)
    closing_stations : list/set, opzionale
    DateTimeFormat : 'datetime64[s]' | 'seconds'
    verbose : bool, se True stampa una tabella ordinata per tempo

    Returns
    -------
    loop_ids_sorted : np.ndarray
        LoopID nell'ordine temporale (cio ordinato con idx_sort).
    idx_sort : np.ndarray
        Indici che ordinano per tempo (per mappare avanti/indietro).
    loop_ids_original_order : np.ndarray
        LoopID riallineati all'ordine di input.
    """

    stations = np.asarray(stations)

    if datetime is None:
        datetime = np.arange( 0, len(stations), dtype='datetime64[s]' )

    datetime = np.asarray(datetime)

    n = len(stations)

    if n == 0:
        return np.array([]), np.array([], dtype=int), np.array([])

    if gobs is not None:
        gobs = np.asarray(gobs)
        if len(gobs) != n:
            raise ValueError("gobs deve avere la stessa lunghezza di stations.")

    if len(exclude_stations) == []:
        exclude_stations = set()
    else:
        exclude_stations = set(exclude_stations)

    # ---- Parsing datetime -> secondi
    dt = datetime
    if dt.dtype.kind in {'U', 'S'}:
        dt = dt.astype('datetime64[s]')

    if 'datetime64' in DateTimeFormat:
        epoch = np.datetime64('1970-01-01T00:00:00')
        time_sec = (dt - epoch) / np.timedelta64(1, 's')
    elif DateTimeFormat == 'seconds':
        time_sec = dt.astype('float64')
    else:
        raise ValueError("DateTimeFormat deve essere 'datetime64[s]' oppure 'seconds'.")

    # ---- Ordino cronologicamente
    idx_sort = np.argsort(time_sec.astype('float64'))
    stations_sorted = stations[idx_sort]
    dt_sorted = (dt if 'datetime64' in str(dt.dtype) else datetime)[idx_sort]
    gobs_sorted = gobs[idx_sort] if gobs is not None else None

    # ---- Assegnazione LoopID "apre e chiude con la stessa stazione"
    loop_ids_sorted = np.full(n, np.nan, dtype=float)
    loop_id = 0
    start_loop = False

    if len(closing_stations) == 0:
        # Find stations repeated more than once
        un_st = np.unique( stations_sorted )
        for sta in un_st:
            count = np.sum(stations_sorted == sta)
            print( sta, count )
            if (count > 1) and (sta not in exclude_stations):
                closing_stations.append(sta)
    print( closing_stations )

    for i, sta in enumerate(stations_sorted):
        if sta in exclude_stations:
            continue

        if ( sta in closing_stations ) and ( start_loop is False ):
            start_loop = True
            loop_id += 1
            loop_ids_sorted[i] = loop_id
        
        elif ( sta in closing_stations ) and ( start_loop is True ):
            loop_ids_sorted[i] = loop_id
            start_loop = False

        elif start_loop is True:
            loop_ids_sorted[i] = loop_id
            
    # ---- Riallineo allordine originale
    loop_ids_original_order = np.full(n, np.nan, dtype=float)
    loop_ids_original_order[idx_sort] = loop_ids_sorted

    # ---- Stampa (opzionale)
    if verbose:
        has_g = gobs_sorted is not None
        header = f"{'LoopID':>6} | {'Station':<12} | {'Datetime (UTC)':<25}" + (" | gobs" if has_g else "")
        print(header)
        print("-" * len(header))

        for i in range(n):
            
            lid = loop_ids_sorted[i]
            lid_str = "NaN" if np.isnan(lid) else f"{int(lid)}"
            dt_str = str(np.datetime_as_string(dt_sorted[i], unit='s'))
            row = f"{lid_str:>6} | {stations_sorted[i]:<12} | {dt_str:<25}"
            
            if has_g:
                row += f" | {gobs_sorted[i]:.6f}"
            print(row)

    return loop_ids_sorted, idx_sort, loop_ids_original_order

# -----------------------------------------------------------------------------
def closure_error( stations, gobs, loop_ids=None, **kwargs ):
    """
    Calcola l'errore di chiusura per ciascun loop, senza usare pandas.
    
    Parametri
    ---------
    stations : array-like
        Nomi stazioni nell'ordine temporale (come ritornati da assign_loop_ids).
    gobs : array-like
        Osservazioni di gravit (Gal).
    loop_ids : array-like
        LoopID associati a ciascuna osservazione.
    
    Returns
    -------
    list of dict
        Lista con LoopID, stazione di partenza e errore di chiusura.
    """
    stations = np.asarray(stations)
    gobs = np.asarray(gobs, dtype=float)
    loop_ids = np.asarray(loop_ids)

    if loop_ids is None:
        _,_,loop_ids = assign_loop_ids( stations, **kwargs )

    close_err = []
    print( loop_ids )
    loops_unici = np.unique(loop_ids[~np.isnan(loop_ids)])

    for loop in loops_unici:

        mask = loop_ids == loop
        st_loop = stations[mask]
        g_loop = gobs[mask]

        if len(st_loop) in [0, 1, 2]:
            continue
        
        dg_sum = 0
        for i in range(len(st_loop) - 1):

            if i == len(st_loop) - 1:
                close_err.append( dg_sum )

            else :
                dg_sum += g_loop[i + 1] - g_loop[i] 

    close_err = np.asarry( close_err )

    return close_err

# -----------------------------------------------------------------------------
# def grav_drift( stations, gobs, datetime=None, date=None, time=None, 
#     deg=1, DateTimeFormat='datetime64[s]', plot=False ) :

#     """
#     Computes a continuous instrumental drift correction curve based on repeated 
#     gravity measurements at the same stations over time. The function estimates 
#     the drift from differences between repeated station readings and fits a 
#     polynomial model to represent the drift behavior across the dataset.

#     Parameters
#     ----------
#     stations : array-like of str
#         List of station names, one for each gravity observation.
    
#     gobs : array-like of float
#         Raw gravity observations (in mGal).

#     datetime : array-like, optional
#         Array of full timestamps (e.g., datetime64 or ISO-format strings), one per observation.
#         If not provided, both `date` and `time` must be specified.

#     date : array-like of str, optional
#         Array of date strings (e.g., '2025-03-06'). Used only if `datetime` is None.

#     time : array-like of str, optional
#         Array of time strings (e.g., '14:30:00'). Used only if `datetime` is None.

#     deg : int, default=1
#         Degree of the polynomial used to fit the drift curve (e.g., 1 = linear drift).

#     DateTimeFormat : str, default='datetime64[s]'
#         Format of the `datetime` values. Accepts 'datetime64[s]' or 'seconds'.
#         Used to determine how timestamps are interpreted internally.

#     plot : bool, default=False
#         If True, generates a plot showing the estimated drift curve and control points 
#         based on repeated measurements.

#     Returns:
#     --------
#     gobs_corr : np.ndarray
#         Drift-corrected gravity observations (gobs - drift).
#     drift_curv : np.ndarray
#         Drift values over time, same shape as gobs.

#     Raises
#     ------
#     ValueError
#         - If `datetime` is None and either `date` or `time` is not provided.
#         - If `DateTimeFormat` is not one of the supported values ('datetime64[s]', 'seconds').

#     Notes
#     -----
#     - The first occurrence of each station is assumed to be the reference for drift calculation.
#     - The drift curve is constructed by comparing repeated station values over time.
#     - A polynomial is then fitted to these estimated drift points to generate a continuous model.
#     - Input timestamps must be valid and properly sortable. ISO8601 or datetime64 formats are recommended.

#     Examples
#     --------
#     >>> drift = grav_drift(
#             stations=['A', 'B', 'A'],
#             gobs=[100.0, 99.5, 100.2],
#             date=['2025-03-06'] * 3,
#             time=['14:00:00', '14:30:00', '15:00:00'],
#             deg=1,
#             plot=True
#         )
#     """

#     # Creating empty variables to append the results
#     drift = []
#     time_drift = []
#     labels= []

#     # Handle datetime input 
#     if datetime is None:
#         if date is None or time is None:
#             raise ValueError("date")

#         # Merge date and time into a single datetime64 array
#         datetime_str = np.char.add(np.array(date, dtype=str), 'T')
#         datetime_str = np.char.add(datetime_str, np.array(time, dtype=str))
#         datetime = datetime_str.astype('datetime64[s]')

#     else:
#         datetime = np.array(datetime)
#         if datetime.dtype.kind in {'U', 'S'}:  # string type
#             datetime = datetime.astype('datetime64[s]')

#     # Creating numpy arrays with the input variables
#     stations = np.array( stations )
#     datetime = np.array( datetime )
#     gobs = np.array( gobs )

#     # Sorting the inmput variables by time
#     idx = datetime.argsort()
#     stations = stations[ idx ]
#     datetime = datetime[ idx ]
#     gobs = gobs[ idx ]

#     # Convert time array to float number with timestamp (i.e., seconds from 01/01/1970)
#     if 'datetime64' in DateTimeFormat :
#         time_num = (datetime - np.datetime64('1970-01-01T00:00:00')) / np.timedelta64(1, 's')
    
#     # Take datetime as seconds
#     elif DateTimeFormat == 'seconds' :
#         time_num = datetime
         
#     # Starting loop to compute the drift fuction
#     for i, r in enumerate( gobs ) :

#         idx = stations[ :i ] == stations[ i ]

#         if np.sum( idx ) != 0 :

#             if len(drift) == 1 :

#                 drift.append( gobs[i] - gobs[:i][idx][0] )

#             else :
#                 print( stations[ i ], time_num[:i][idx][0], i )
#                 f = utl.sp.interpolate.interp1d( time_drift, drift, kind='linear')
#                 dlv = f( time_num[:i][idx][0] )

#                 if time_num[:i][idx][0] not in time_drift :
                    
#                     drift.append( dlv )
#                     time_drift.append( time_num[:i][idx][0] )
#                     labels.append( stations[:i][idx][0] )
#                 drift.append( gobs[i] - ( gobs[:i][idx][0] - dlv ) )

#             time_drift.append( time_num[i] )
#             labels.append( stations[i] ) 

#         else :
#             if i == 0 :
#                 drift.append( 0 )
#                 time_drift.append( time_num[i] )
#                 labels.append( stations[i] )

#     # Creating numpy arrays with the output variables
#     labels = np.array( labels )
#     time_drift = np.array( time_drift )
#     drift = np.array( drift )

#     # Sorting the output variables by time
#     idx = time_drift.argsort()
#     labels = labels[ idx ]
#     time_drift = time_drift[ idx ]
#     drift = drift[ idx ]

#     t0 = time_drift[0]
#     dlp = np.polyfit( time_drift-t0, drift, deg=deg )
#     drift_curv = np.polyval( dlp, time_num-t0 )


#     time_drift = np.array( [ dt.datetime.fromtimestamp( i, dt.timezone.utc ) for i in time_drift ] )

#     # Apply drift correction
#     gobs_corr = gobs - drift_curv

#     # Plotting the drift function if requested
#     if plot is True : 

#         plt.plot( time_drift, drift, marker='o', 
#             linestyle='dashed', label='Drift Obs. Values' )

#         plt.plot( datetime, drift_curv, label='Drift Fit', color='red' )

#         # Annotate each label, offsetting to avoid overlap and covering points
#         for i, txt in enumerate(labels):
#             # Offset each annotation slightly above the point, and stagger horizontally if needed
#             plt.annotate(
#             txt,
#             (time_drift[i], drift[i]),
#             xytext=(5, 10 + (i % 3) * 10),  # Offset x by 5, y by 10/20/30 pixels
#             textcoords='offset points',
#             ha='left',
#             va='bottom',
#             fontsize=8,
#             bbox=dict(boxstyle="round,pad=0.2", fc="white", alpha=0.7),
#             arrowprops=dict(arrowstyle="-", color="gray", lw=0.5)
#             )

#         plt.gcf().autofmt_xdate() 

#     return gobs_corr, drift_curv

# -----------------------------------------------------------------------------
def grav_drift(
    stations, gobs, datetime=None, date=None, time=None,
    deg=1, DateTimeFormat='datetime64[s]', plot=False):

    """
    Computes a continuous instrumental drift correction curve based on repeated
    gravity measurements at the same stations over time. The function estimates
    the drift from differences between repeated station readings and fits a
    polynomial model to represent the drift behavior across the dataset.
    
    Parameters
    ----------

    stations : array-like of str
        List of station names, one for each gravity observation.
    gobs : array-like of float
        Raw gravity observations (in mGal).
    datetime : array-like, optional
        Array of full timestamps (e.g., datetime64 or ISO-format strings), one per observation.
        If not provided, both `date` and `time` must be specified.
    date : array-like of str, optional
        Array of date strings (e.g., '2025-03-06'). Used only if `datetime` is None.
    time : array-like of str, optional
        Array of time strings (e.g., '14:30:00'). Used only if `datetime` is None.
    deg : int, default=1
        Degree of the polynomial used to fit the drift curve (e.g., 1 = linear drift).
    DateTimeFormat : str, default='datetime64[s]'
        Format of the `datetime` values. Accepts 'datetime64[s]' or 'seconds'.
    plot : bool, default=False
        If True, generates a plot showing the estimated drift curve and control points  
        based on repeated measurements.
    
    Returns
    -------

    gobs_corr : np.ndarray
        Drift-corrected gravity observations (gobs - drift).
    drift_curv : np.ndarray
        Drift values over time, same shape as gobs.
    
    """

    # Build datetime
    if datetime is None:
        if date is None or time is None:
            raise ValueError("If 'datetime' is None, 'date' and 'time' must be provided.")
        datetime_str = np.char.add(np.array(date, dtype=str), 'T')
        datetime_str = np.char.add(datetime_str, np.array(time, dtype=str))
        datetime = datetime_str.astype('datetime64[s]')
    else:
        datetime = np.array(datetime)
        if datetime.dtype.kind in {'U', 'S'}:
            datetime = datetime.astype('datetime64[s]')

    # Convert to seconds
    if 'datetime64' in DateTimeFormat:
        time_sec = (datetime - np.datetime64('1970-01-01T00:00:00')) / np.timedelta64(1, 's')
    elif DateTimeFormat == 'seconds':
        time_sec = datetime.astype(float)
    else:
        raise ValueError("Unsupported DateTimeFormat.")

    # Sort all data by time
    stations = np.array(stations)
    gobs = np.array(gobs)
    time_sec = np.array(time_sec)
    idx_sort = np.argsort(time_sec)
    stations = stations[idx_sort]
    gobs = gobs[idx_sort]
    time_sec = time_sec[idx_sort]

    # Identify repeated stations
    unique_stations = np.unique(stations)
    rep_stations = []
    for s in unique_stations:
        mask = stations == s
        if np.sum(mask) > 1:
            rep_stations.append(s)

    for i, s in enumerate( rep_stations ):
        
        mask = stations == s

        if i == 0 :
            
            shift = gobs[mask][0]
            drift = gobs[mask] - shift
            time_drift = time_sec[mask]
            st_drift = stations[mask]

        else :

            f = utl.sp.interpolate.interp1d( time_drift, drift, kind='linear')
            shift = np.abs( gobs[mask][0] - f( time_sec[mask][0] ) )

            new_drift = gobs[mask] - shift
            new_time_drift = time_sec[mask]
            drift = np.concatenate((drift, new_drift))
            time_drift = np.concatenate((time_drift, new_time_drift))
            st_drift = np.concatenate((st_drift, stations[mask]))

            # Sort the drift and time arrays
            sort_idx = np.argsort(time_drift)
            drift = drift[sort_idx]
            time_drift = time_drift[sort_idx]
            st_drift = st_drift[sort_idx]


    t0 = time_drift[0]
    pf = np.polyfit( time_drift, drift, deg=deg )
    drift_curv = np.polyval( pf, time_sec )

    # Apply drift correction
    gobs_corr = gobs - drift_curv


    # Convert time to timezone-aware UTC datetime objects for plotting (if needed)
    time_drift_dt = np.array([dt.datetime.fromtimestamp(t, dt.timezone.utc) for t in time_drift])
    time_dt = np.array([dt.datetime.fromtimestamp(t, dt.timezone.utc) for t in time_sec])

    if plot : 
        plt.figure(figsize=(10, 6))

        for s in np.unique(rep_stations):
            mask = st_drift == s
            plt.plot(time_drift_dt[mask], drift[mask], marker='o', linestyle='-', label=f'Station {s}')

        plt.plot(time_drift_dt, drift, linestyle='dotted', label="Drift curve", linewidth=2, c='k' ) 

        plt.plot(time_dt, drift_curv, label='Drift Fit', c='k', linewidth=2, linestyle='--')
        

        plt.gcf().autofmt_xdate() 

        plt.legend() 

    return gobs_corr, drift_curv

# -----------------------------------------------------------------------------
def network_adjust(observations, n_stations, n_links=1, drift=False, k=False):
    """
    Performs least squares adjustment of a gravity network, with optional estimation
    of instrument scale factors (k) and time drift terms (d).

    Parameters
    ----------
    observations : list of tuples
        Each observation is a tuple: (i, j, g_obs, T, link_id, weight)
        - i, j: indices of station pairs (0-based)
        - g_obs: observed gravity difference between station j and i
        - T: time interval between observations (in seconds)
        - link_id: ID of the loop (typically 0 if the network is unified)
        - weight: weight of the observation (1/)

    n_stations : int
        Number of unique stations in the network.

    n_links : int, default=1
        Number of network blocks or loops (each with independent k and/or d if enabled).

    drift : bool, default=False
        If True, estimate drift terms d_m for each loop m.

    k : bool, default=False
        If True, estimate scale factors k_m for each loop m.

    Returns
    -------
    X : np.ndarray
        Vector of estimated unknowns. Format:
        - First n_stations elements: gravity values at each station
        - Followed by n_links scale factors k_m if k=True
        - Followed by n_links drift terms d_m if drift=True

    V : np.ndarray
        Residual vector of the adjustment.

    sigma0 : float
        A posteriori standard deviation of unit weight.

    Notes
    -----
    The number of estimated parameters varies depending on the options:
        total_parameters = n_stations + (k * n_links) + (drift * n_links)

    The design matrix A is built dynamically based on enabled options.
    """

    n_obs = len(observations)
    n_params = n_stations + (n_links if k else 0) + (n_links if drift else 0)

    A = np.zeros((n_obs, n_params))
    L = np.zeros(n_obs)
    P = np.zeros((n_obs, n_obs))

    for idx, (i, j, delta_g, delta_t, link_id, weight) in enumerate(observations):
        # Gravity values
        A[idx, i] = 1          # +g_i
        A[idx, j] = -1         # -g_j

        # Scale factor term (optional)
        if k:
            A[idx, n_stations + link_id] = -delta_g

        # Drift term (optional)
        if drift:
            offset = n_stations + (n_links if k else 0)
            A[idx, offset + link_id] = -delta_t

        # Observation and weight
        L[idx] = delta_g
        P[idx, idx] = weight

    # Normal equation system: N X = u
    N = A.T @ P @ A
    u = -A.T @ P @ L

    # Solve system (must be full rank)
    X = np.linalg.solve(N, u)

    # Residuals and variance factor
    V = A @ X + L
    sigma0_sq = (V.T @ P @ V) / (n_obs - n_params)
    sigma0 = np.sqrt( sigma0_sq )

    # Covariance matrix and standard errors (only for gravity estimates)
    Cov = sigma0_sq * np.linalg.inv(N)
    std_errors = np.sqrt(np.diag(Cov[:n_stations, :n_stations]))

    return X, V, sigma0, std_errors

# -----------------------------------------------------------------------------
def grav_net_lsqadj(
    stations, gobs, datetime=None, date=None, time=None,
    DateTimeFormat='datetime64[s]', weight=1.0, grav_abs=[], 
    drift=False, k=False, n_links=1, plot=False) :

    """
    Compensates a gravity network using observed gravity values, timestamps,
    and station names. Allows optional absolute reference stations to be fixed.

    Parameters
    ----------
    stations : array-like of str
        Station names corresponding to each gravity observation.

    gobs : array-like of float
        Observed gravity values (in mGal).

    datetime : array-like, optional
        Full datetime array in numpy.datetime64 format or ISO strings.
        Required unless `date` and `time` are provided.

    date : array-like of str, optional
        Date strings (e.g., '2025-03-06'). Used if `datetime` is not provided.

    time : array-like of str, optional
        Time strings (e.g., '14:30:00'). Used if `datetime` is not provided.

    DateTimeFormat : str, default='datetime64[s]'
        Format of the input time. Accepts 'datetime64[s]' or 'seconds'.

    weight : float, default=1.0
        Weight applied to all observation pairs (1/).

    grav_abs : list of str, optional
        List of station names to fix as absolute gravity reference (e.g., ['CRS_Abs']).
        If empty, the first station in time is fixed to 0.

    Returns
    -------
    g_est : np.ndarray
        Estimated gravity values for each unique station.

    V : np.ndarray
        Residual vector of the least squares adjustment.

    sigma0 : float
        A posteriori standard deviation of unit weight.

    unique_stations : np.ndarray
        List of unique station names in the same order as `g_est`.

    Raises
    ------
    ValueError
        If no valid datetime is provided or DateTimeFormat is unsupported.

    Notes
    -----
    The adjustment uses pairwise g and T values to build observation equations.
    A least squares solver is used to compute the best-fit values.
    """

    # Handle datetime from separate date/time if needed
    if datetime is None:
        if date is None or time is None:
            raise ValueError("If 'datetime' is None, 'date' and 'time' must be provided.")
        datetime_str = np.char.add(np.array(date, dtype=str), 'T')
        datetime_str = np.char.add(datetime_str, np.array(time, dtype=str))
        datetime = datetime_str.astype('datetime64[s]')
    else:
        datetime = np.array(datetime)
        if datetime.dtype.kind in {'U', 'S'}:
            datetime = datetime.astype('datetime64[s]')

    # Convert datetime to seconds since epoch
    if 'datetime64' in DateTimeFormat:
        time_sec = (datetime - np.datetime64('1970-01-01T00:00:00')) / np.timedelta64(1, 's')
    elif DateTimeFormat == 'seconds':
        time_sec = datetime.astype(float)
    else:
        raise ValueError("Unsupported DateTimeFormat.")

    # Sort all inputs chronologically
    stations = np.array(stations)
    gobs = np.array(gobs)
    idx = np.argsort(time_sec)
    stations = stations[idx]
    gobs = gobs[idx]
    time_sec = time_sec[idx]

    # Map station names to unique indices
    unique_stations = np.unique(stations)
    station_map = {s: i for i, s in enumerate(unique_stations)}
    n_stations = len(unique_stations)

    # Build observations as g and T between sequential measurements
    observations = []
    edge_log = []  # Per controllo loop
    for i in range(len(gobs) - 1):
        sta_i, sta_j = stations[i], stations[i + 1]
        if sta_i != sta_j:
            idx_i = station_map[sta_i]
            idx_j = station_map[sta_j]
            delta_g = gobs[i + 1] - gobs[i]
            delta_t = time_sec[i + 1] - time_sec[i]
            observations.append((idx_i, idx_j, delta_g, delta_t, 0, weight))
            edge_log.append((sta_i, sta_j, delta_g))

    # If no absolute reference is provided, use first station
    if not grav_abs:
        grav_abs = [stations[0]]

    # Add pseudo-observations to fix gravity at absolute stations
    # for absi, abs_sta in enumerate( grav_abs ):
    #     if abs_sta[0] in station_map:
    #         idx_fixed = station_map[abs_sta[0]]
    #         # Strong constraint
    #         observations.append((idx_fixed, idx_fixed, 0.0, 0.0, 0, 1e6))

    for abs_sta in grav_abs:
        if isinstance(abs_sta, str):
            abs_sta = [abs_sta]

        name = abs_sta[0]
        g_abs = abs_sta[1] if len(abs_sta) > 1 else 0.0
        g_err = abs_sta[2] if len(abs_sta) > 2 else 1e-6  # peso molto alto se non specificato

        if name in station_map:
            idx_fixed = station_map[name]
            observations.append((idx_fixed, idx_fixed, g_abs, 0.0, 0, 1.0 / g_err**2))


    # Solve the network adjustment
    X, residuals, sigma0, std_errors = network_adjust( observations, n_stations=n_stations, 
        n_links=n_links, drift=drift, k=k )

    # Extract gravity estimates
    g_est = X[:n_stations]

    # If no absolute stations, shift so the first station = 0
    if not grav_abs:
        g_est -= g_est[0]

    if plot is True:
        
        plt.hist( residuals, bins='auto', edgecolor='k')
        plt.title("Residuals distribution")
        plt.xlabel("Residual (mGal)")
        plt.ylabel("Frequency")
        plt.grid(True)

    return g_est, unique_stations, residuals, std_errors, sigma0

# -----------------------------------------------------------------------------
def earth_tides( lat, lon, z=0, datetime=None, 
                 yy=None, mm=None, dd=None, 
                 h=None, m=None, s=None ):
    """
    Calculate the Earth tides at a given location and time using Longman 1959.

    Parameters:
    lat (float): Latitude of the location.
    lon (float): Longitude of the location.
    z (float, optional): Elevation of the location (default is 0).
    datetime (datetime.datetime or numpy.ndarray or numpy.datetime64, optional): 
        Date and time of the calculation.
        If not provided, the current date and time will be used.
    yy (float, optional): Year component of the datetime 
        (required if datetime is not provided).
    mm (float, optional): Month component of the datetime 
        (required if datetime is not provided).
    dd (float, optional): Day component of the datetime 
        (required if datetime is not provided).
    h (float, optional): Hour component of the datetime 
        (required if datetime is not provided).
    m (float, optional): Minute component of the datetime 
        (required if datetime is not provided).
    s (float, optional): Second component of the datetime 
        (required if datetime is not provided).

    Returns:
    float or numpy.ndarray: Earth tides at the given location and time.

    NB.
    The Earth tides values must be REMOVED (-) from the measured gravity for 
    gravity data reduction.
    """

    if datetime is None:
        datetime = utl.combine64( years=yy, 
                                  months=mm, days=dd, 
                                  hours=h, minutes=m, 
                                  seconds=s)

    if type(datetime) in (np.ndarray, np.datetime64):
        datetime = datetime.tolist()
        if type(datetime) is not list:
            datetime = [datetime]

    # Convert inputs to numpy arrays
    lat = np.full(np.size(datetime), lat)
    lon = np.full(np.size(datetime), lon)
    z = np.full(np.size(datetime), z)

    tides = np.full(lat.shape, np.nan)

    for i, _ in enumerate(tides):
        model = TideModel()
        tides[i] = model.solve_longman(lat[i], lon[i], z[i], datetime[i])[2]

    if np.size(tides) == 1:
        tides = tides[0]

    return tides

# -----------------------------------------------------------------------------
class TideModel():
    """
    Class to encapsulate the Longman 1959 tide model.

    Exemple
    model = TideModel() # Make a model object
    model.increment = 60*10 # Run every 10 minutes [seconds]
    model.latitude = 40.7914 # Station Latitude
    model.longitude = 282.1414 # Station Longitude
    model.altitude = 370. # Station Altitude [meters]
    model.start_time = datetime(2015,4,23,0,0,0)
    model.duration = 7 # Model run duration [days]
    model.run_model() # Do the run
    model.write('output.txt') # Save results to text file
    model.plot() # Make a quick-dirty-plot

    ----------
    References 
    This python class has been adapted from 
    the original pubblic code of John Leeman (2017)
    LongmanTide.
    The code was downloaded from the github repository:
    https://github.com/jrleeman/LongmanTide.git at 2024-01-31
    """

    def __init__(self):
        """Initialize the model object."""
        self.name = 'Model'
        self.results = namedtuple('results', ['model_time', 'gravity_moon',
                                              'gravity_sun', 'gravity_total'])
        self.results.model_time = []
        self.results.gravity_moon = []
        self.results.gravity_sun = []
        self.results.gravity_total = []

    def calculate_julian_century(self, timestamp):
        """
        Calculate the julian century and hour.
        Take a datetime object and calculate the decimal Julian century and
        floating point hour. This is in reference to noon on December 31,
        1899 as stated in the Longman paper.
        Parameters
        ----------
        timestamp: datetime
            Time stamp to convert
        Returns
        -------
        float, float
            Julian century and hour
        """
        origin_date = datetime(1899, 12, 31, 12, 00, 00)  # Noon Dec 31, 1899
        dt = timestamp - origin_date
        days = dt.days + dt.seconds / 3600. / 24.
        decimal_julian_century = days / 36525
        julian_hour = (timestamp.hour + timestamp.minute / 60. +
                       timestamp.second / 3600.)
        return decimal_julian_century, julian_hour

    def solve_longman(self, lat, lon, alt, time):
        """
        Solve the tide model.
        Given the location and datetime object, computes the current
        gravitational tide and associated quantities. Latitude and longitude
        and in the traditional decimal notation, altitude is in meters, time
        is a datetime object.
        Parameters
        ----------
        lat : float
            latitude (in degrees)
        lon : float
            longitude (in degrees)
        alt : float
            altitude (in meters)
        time : datetime
            time at which to solve the model
        Returns
        -------
        float, float, float
            lunar, solar, and total gravitational tides
        """
        T, t0 = self.calculate_julian_century(time)

        if t0 < 0:
            t0 += 24.
        if t0 >= 24:
            t0 -= 24.

        mu = 6.673e-8  # Newton's gravitational constant
        M = 7.3537e25  # Mass of the moon in grams
        S = 1.993e33  # Mass of the sun in grams
        e = 0.05490  # Eccentricity of the moon's orbit
        m = 0.074804  # Ratio of mean motion of the sun to that of the moon
        # Mean distance between the centers of the earth and the moon
        c = 3.84402e10
        # Mean distance between centers of the earth and sun in cm
        c1 = 1.495e13
        h2 = 0.612  # Love parameter
        k2 = 0.303  # Love parameter
        a = 6.378270e8  # Earth's equitorial radius in cm
        i = 0.08979719  # (i) Inclination of the moon's orbit to the ecliptic
        # Inclination of the Earth's equator to the ecliptic 23.452 degrees
        omega = np.radians(23.452)
        # For some reason his lat/lon is defined with W as + and E as -
        L = -1 * lon
        lamb = np.radians(lat)  # (lambda) Latitude of point P
        H = alt * 100.  # (H) Altitude above sea-level of point P in cm

        # Lunar Calculations
        # (s) Mean longitude of moon in its orbit reckoned
        # from the referred equinox
        s = (4.72000889397 + 8399.70927456 * T + 3.45575191895e-05 * T * T +
             3.49065850399e-08 * T * T * T)
        # (p) Mean longitude of lunar perigee
        p = (5.83515162814 + 71.0180412089 * T + 0.000180108282532 * T * T +
             1.74532925199e-07 * T * T * T)
        # (h) Mean longitude of the sun
        h = 4.88162798259 + 628.331950894 * T + 5.23598775598e-06 * T * T
        # (N) Longitude of the moon's ascending node in its orbit
        # reckoned from the referred equinox
        N = (4.52360161181 - 33.757146295 * T + 3.6264063347e-05 * T * T +
             3.39369576777e-08 * T * T * T)
        # (I) Inclination of the moon's orbit to the equator
        I = np.arccos(np.cos(omega)*np.cos(i) - np.sin(omega)*np.sin(i)*np.cos(N))
        # (nu) Longitude in the celestial equator of its intersection
        # A with the moon's orbit
        nu = np.arcsin(np.sin(i)*np.sin(N)/np.sin(I))
        # (t) Hour angle of mean sun measured west-ward from
        # the place of observations
        t = np.radians(15. * (t0 - 12) - L)

        # (chi) right ascension of meridian of place of observations
        # reckoned from A
        chi = t + h - nu
        # cos(alpha) where alpha is defined in eq. 15 and 16
        cos_alpha = np.cos(N) * np.cos(nu) + np.sin(N) * np.sin(nu) * np.cos(omega)
        # sin(alpha) where alpha is defined in eq. 15 and 16
        sin_alpha = np.sin(omega) * np.sin(N) / np.sin(I)
        # (alpha) alpha is defined in eq. 15 and 16
        alpha = 2 * np.arctan(sin_alpha / (1 + cos_alpha))
        # (xi) Longitude in the moon's orbit of its ascending
        # intersection with the celestial equator
        xi = N - alpha

        # (sigma) Mean longitude of moon in radians in its orbit
        # reckoned from A
        sigma = s - xi
        # (l) Longitude of moon in its orbit reckoned from its ascending
        # intersection with the equator
        l = (sigma + 2 * e * np.sin(s - p) + (5. / 4) * e * e * np.sin(2 * (s - p)) +
             (15. / 4) * m * e * np.sin(s - 2 * h + p) + (11. / 8) *
             m * m * np.sin(2 * (s - h)))

        # Sun
        # (p1) Mean longitude of solar perigee
        p1 = (4.90822941839 + 0.0300025492114 * T + 7.85398163397e-06 *
              T * T + 5.3329504922e-08 * T * T * T)
        # (e1) Eccentricity of the Earth's orbit
        e1 = 0.01675104 - 0.00004180 * T - 0.000000126 * T * T
        # (chi1) right ascension of meridian of place of observations
        # reckoned from the vernal equinox
        chi1 = t + h
        # (l1) Longitude of sun in the ecliptic reckoned from the
        # vernal equinox
        l1 = h + 2 * e1 * np.sin(h - p1)
        # cosine(theta) Theta represents the zenith angle of the moon
        cos_theta = (np.sin(lamb) * np.sin(I) * np.sin(l) + np.cos(lamb) * (np.cos(0.5 * I)**2
                     * np.cos(l - chi) + np.sin(0.5 * I)**2 * np.cos(l + chi)))
        # cosine(phi) Phi represents the zenith angle of the run
        cos_phi = (np.sin(lamb) * np.sin(omega) * np.sin(l1) + np.cos(lamb) *
                   (np.cos(0.5 * omega)**2 * np.cos(l1 - chi1) +
                   np.sin(0.5 * omega)**2 * np.cos(l1 + chi1)))

        # Distance
        # (C) Distance parameter, equation 34
        C = np.sqrt(1. / (1 + 0.006738 * np.sin(lamb)**2))
        # (r) Distance from point P to the center of the Earth
        r = C * a + H
        # (a') Distance parameter, equation 31
        aprime = 1. / (c * (1 - e * e))
        # (a1') Distance parameter, equation 31
        aprime1 = 1. / (c1 * (1 - e1 * e1))
        # (d) Distance between centers of the Earth and the moon
        d = (1. / ((1. / c) + aprime * e * np.cos(s - p) + aprime * e * e *
             np.cos(2 * (s - p)) + (15. / 8) * aprime * m * e * np.cos(s - 2 * h + p)
             + aprime * m * m * np.cos(2 * (s - h))))
        # (D) Distance between centers of the Earth and the sun
        D = 1. / ((1. / c1) + aprime1 * e1 * np.cos(h - p1))

        # (gm) Vertical componet of tidal acceleration due to the moon
        gm = ((mu * M * r / (d * d * d)) * (3 * cos_theta**2 - 1) + (3. / 2) *
              (mu * M * r * r / (d * d * d * d)) *
              (5 * cos_theta**3 - 3 * cos_theta))
        # (gs) Vertical componet of tidal acceleration due to the sun
        gs = mu * S * r / (D * D * D) * (3 * cos_phi**2 - 1)

        love = (1 + h2 - 1.5 * k2)
        g0 = - (gm + gs) * 1e3 * love

        return gm * 1e3 * love, gs * 1e3 * love, g0

    def run_model(self):
        """
        Run the model for a range of times.
        Runs the tidal model beginning at start_time with time steps of
        increment seconds for days.
        """
        self.n_steps = int(24 * self.duration * 3600 / self.increment)

        for i in np.arange(self.n_steps):
            time_at_step = (self.start_time +
                            i * timedelta(seconds=self.increment))
            gm, gs, g = self.solve_longman(self.latitude, self.longitude,
                                           self.altitude, time_at_step)
            self.results.model_time.append(time_at_step)
            self.results.gravity_moon.append(gm)
            self.results.gravity_sun.append(gs)
            self.results.gravity_total.append(g)

    def plot(self):
        """
        Plot the model results.
        Make a simple plot of the gravitational tide results from the
        model run.
        """
        fig = plt.figure('Tidal Model')
        ax1 = plt.subplot(111)
        ax1.set_xlabel(r'Date', fontsize=18)
        ax1.set_ylabel(r'Anomaly [mGal]', fontsize=18)
        ax1.tick_params(axis='both', which='major', labelsize=16)
        ax1.plot_date(self.results.model_time, self.results.gravity_total,
                      '-k', linewidth=2)
        plt.show()
        return fig, ax1

    def write(self, fname):
        """
        Write model results to file.
        Write results out of a file for later analysis or reading into another
        method for analysis/correction of data.
        Parameters
        ----------
        fname: string
            name of file to save
        """
        t_string = datetime.strftime(self.start_time, '%Y-%m-%dT%H:%M:%S')
        f = open(fname, 'w')
        f.write('Station latitude: {self.latitude}\n')
        f.write('Station longitude: {self.longitude}\n')
        f.write('Station altitude [m]: {self.altitude}\n')
        f.write('Time Increment [s]: {self.increment}\n')
        f.write('Start Time: {t_string}\n')
        f.write('Duration [days]: {self.duration}\n')
        f.write('\nTime,Lunar,Solar,Total\n')
        f.write('YYYY-MM-DDTHH:MM:SS\tmGal\tmGal\tmGal\n')

        for i in np.arange(self.n_steps):
            t_string = datetime.strftime(self.results.model_time[i],
                                         '%Y-%m-%dT%H:%M:%S')
            f.write('{}\t{}\t{}\t{}\n'.format(t_string,
                                              self.results.gravity_moon[i],
                                              self.results.gravity_sun[i],
                                              self.results.gravity_total[i]))
        f.close()

# -----------------------------------------------------------------------------
def read_burris_file( file_path, fprint=False, **fprintargs ) :

    with open(file_path, 'r') as file:

        lines = file.readlines()
    
    file.close()

    for i, line in enumerate( lines ) :

        if i == 0 :
            # Read the first line to get the headers
            headers = line.strip().split(',')
            # Initialize a dictionary with headers as keys and empty lists as values
            data_dict = {header: [] for header in headers}
        
        # Iterate over each line in the file
        else :
            # Split the line by comma and strip to remove any leading/trailing whitespace
            values = line.split(',')
            # Iterate over each value and its corresponding header
            for header, value in zip(headers, values):
                # Append the value to the correct list in the dictionary
                data_dict[header].append( value )

    # Check if all dict. elements have the same size
    keys_list = list( data_dict.keys() )
    for i, k in enumerate( keys_list ) :
        if i == 0 :
            continue
        if len( data_dict[k] ) != len( data_dict[ keys_list[i-1 ] ] ) :
            raise ValueError( 'The number of elements in the columns are not the same' )

    # Convert the lists to numpy arrays
    for k in data_dict.keys() :
        if 'ObsG' in k :
            data_dict[k] = np.array( data_dict[k], dtype=float )
        elif 'Dial' in k :
            data_dict[k] = np.array( data_dict[k], dtype=float )
        elif 'Feedback' in k :
            data_dict[k] = np.array( data_dict[k], dtype=float )
        elif 'Earthtide' in k :
            data_dict[k] = np.array( data_dict[k], dtype=float )
        elif 'Level Corr' in k :
            data_dict[k] = np.array( data_dict[k], dtype=float )
        elif 'Temp Corr' in k :
            data_dict[k] = np.array( data_dict[k], dtype=float )
        elif 'Beam Err' in k :
            data_dict[k] = np.array( data_dict[k], dtype=float )
        elif 'Height' in k :
            data_dict[k] = np.array( data_dict[k], dtype=float )
        elif 'Elev' in k :
            data_dict[k] = np.array( data_dict[k], dtype=float )
        elif 'Lat' in k :
            data_dict[k] = np.array( data_dict[k], dtype=float )
        elif 'Lon' in k :
            data_dict[k] = np.array( data_dict[k], dtype=float )
        elif 'Elapsed Ti' in k :
            data_dict[k] = np.array( data_dict[k], dtype='float' )
        elif 'Standard D' in k :
            data_dict[k] = np.array( data_dict[k], dtype='float' )
        elif 'Temperature' in k :
            data_dict[k] = np.array( data_dict[k], dtype='float' )
        else :
            data_dict[k] = np.array( data_dict[k], dtype=str )

    # Split the Date column into year, month, and day
    data_dict['yy'] = np.zeros( np.size( data_dict['Date'] ), dtype=int )
    data_dict['mm'] = np.zeros( np.size( data_dict['Date'] ), dtype=int )
    data_dict['dd'] = np.zeros( np.size( data_dict['Date'] ), dtype=int )
    for i, _ in enumerate( data_dict['Date'] ) :
        data_dict['yy'][i] = data_dict['Date'][i].split('/')[0]
        data_dict['mm'][i] = data_dict['Date'][i].split('/')[1]
        data_dict['dd'][i] = data_dict['Date'][i].split('/')[2]

    # Split the Time column into hour, minute, and second
    data_dict['h'] = np.zeros( np.size( data_dict['Time'] ), dtype=int )
    data_dict['m'] = np.zeros( np.size( data_dict['Time'] ), dtype=int )
    data_dict['s'] = np.zeros( np.size( data_dict['Time'] ), dtype=float )
    for i, _ in enumerate( data_dict['Time'] ) :
        data_dict['h'][i] = data_dict['Time'][i].split(':')[0]
        data_dict['m'][i] = data_dict['Time'][i].split(':')[1]
        data_dict['s'][i] = data_dict['Time'][i].split(':')[2]

    if fprint is True :
        _ = utl.print_table( data_dict, **fprintargs )

    return data_dict

# -----------------------------------------------------------------------------
def write_burris_file( station, date, g, lon, lat, 
                       elev=0, tide_corr=0, meter='D45', 
                       oper='abc', feedback=0, dial_setting=50000,
                       path_name='burris.dat', level_corr=0, 
                       temp_corr=0, beam_err=0, height=0 ) :
    
   if np.issubdtype( date.dtype, np.datetime64 ) : 
        date = np.datetime_as_string( date, unit='s')
        date = np.char.replace( date, '-', '/' )
        date = np.char.replace( date, 'T', ' ' )

   if type( lon ) in ( int, float ) :
        lon = np.full( np.size(g), lon )

   if type( lat ) in ( int, float ) :
        lat = np.full( np.size(g), lat ) 

   if type( elev ) in ( int, float ) :
        elev = np.full( np.size(g), elev ) 

   if type( oper ) in ( int, float, str ) :
        oper = np.full( np.size(g), oper ) 

   if type( meter ) in ( int, float, str ) :
        meter = np.full( np.size(g), meter )  

   if type( feedback ) in ( int, float, str ) :
        feedback = np.full( np.size(g), feedback ) 

   if type( dial_setting ) in ( int, float, str ) :
        dial_setting = np.full( np.size(g), dial_setting ) 

   if type( tide_corr ) in ( int, float, str ) :
        tide_corr = np.full( np.size(g), tide_corr ) 

   if type( level_corr ) in ( int, float, str ) :
        level_corr = np.full( np.size(g), level_corr )  

   if type( temp_corr ) in ( int, float, str ) :
        temp_corr = np.full( np.size(g), temp_corr )  

   if type( beam_err ) in ( int, float, str ) :
        beam_err = np.full( np.size(g), beam_err )  

   if type( height ) in ( int, float, str ) :
        height = np.full( np.size(g), height )         

   with open( path_name, 'w' ) as f : 
        for i, gi in enumerate( g ) :
             f.write( f"{station[i]} {oper[i]} {meter[i]} "+ 
                      f"{date[i]} {gi:>.5f} {dial_setting[i]} {feedback[i]} "+ 
                      f"{tide_corr[i]} {level_corr[i]} {temp_corr[i]} {beam_err[i]} {height[i]} "+
                      f"{elev[i]} {lat[i]:.5f} {lon[i]:.5f} \n" )
             
   f.close()

   return path_name

# -----------------------------------------------------------------------------
def Tvt_mag( mag, datetime64, idx=None, hint=[0, 4], deg=1, plot=True, sbplt=111 ) :

    H = np.array( [ int( i.astype(str).split(':')[0].split('T')[-1] ) for i in datetime64 ] )
    idx = ( H >= hint[0] ) & ( H <= hint[1] )

    fit = np.polyval( np.polyfit( datetime64.astype(float)[idx], mag[idx], deg ), 
                      datetime64.astype(float) )

    Ttv = mag - fit

    if plot is True :

        host = plt.subplot( sbplt )
        plt.title('Magnetic transient variation, Ttv [nT]')
        ax2 = host.twinx()
        plt.plot( datetime64, mag )
        plt.plot( datetime64, fit ) 
        host.plot( datetime64, Ttv, linestyle='--' ) 

    return Ttv

# ----------------------------------------------------------------------------
def ALegendreFS( theta, nmax, mmax=None ):
    """ 
    Calculate Schmidt semi-normalized associated Legendre functions

    Calculations based on recursive algorithm found in 
    "Spacecraft Attitude Determination and Control" by James Richard Wertz
    
    Parameters
    ----------
    theta : array
        Array of colatitudes in degrees
    keys: iterable
        list of spherical harmnoic degree and order, tuple (n, m) for each 
        term in the expansion

    Returns
    -------
    P : array 
        Array of Legendre functions, with shape (theta.size, len(keys)). 
    dP : array
        Array of dP/dtheta, with shape (theta.size, len(keys))
    """

    if mmax is None :
        mmax = nmax

    theta = theta.flatten()[:, np.newaxis]
    theta_rad = np.radians(theta)

    P = {}
    dP = {}
    sinth = np.sin(theta_rad)
    costh = np.cos(theta_rad)

    # Initialize Schmidt normalization
    S = {}
    S[0, 0] = 1.

    # initialize the functions:
    for n in range(nmax +1):
        for m in range(nmax + 1):
            P[n, m] = np.zeros_like(theta, dtype = np.float64)
            dP[n, m] = np.zeros_like(theta, dtype = np.float64)

    nm_list = []
    P[0, 0] = np.ones_like(theta, dtype = np.float64)
    for n in range(1, nmax +1):
        for m in range(0, min([n + 1, mmax + 1])):
            # do the legendre polynomials and derivatives
            if n == m:
                P[n, n]  = sinth * P[n - 1, m - 1]
                dP[n, n] = sinth * dP[n - 1, m - 1] + costh * P[n - 1, n - 1]
            else:

                if n == 1:
                    Knm = 0.
                    P[n, m]  = costh * P[n -1, m]
                    dP[n, m] = costh * dP[n - 1, m] - sinth * P[n - 1, m]

                elif n > 1:
                    Knm = ((n - 1)**2 - m**2) / ((2*n - 1)*(2*n - 3))
                    P[n, m]  = costh * P[n -1, m] - Knm*P[n - 2, m]
                    dP[n, m] = costh * dP[n - 1, m] - sinth * P[n - 1, m] - Knm * dP[n - 2, m]

            # compute Schmidt normalization
            if m == 0:
                S[n, 0] = S[n - 1, 0] * (2.*n - 1)/n
            else:
                S[n, m] = S[n, m - 1] * np.sqrt((n - m + 1)*(int(m == 1) + 1.)/(n + m))

            nm_list.append( [ n, m ] )


    # now apply Schmidt normalization
    for n in range(1, nmax + 1):
        for m in range(0, min([n + 1, mmax + 1])):
            P[n, m]  *= S[n, m]
            dP[n, m] *= S[n, m]
    Pmat  = np.hstack(tuple(P[nm[0],nm[1]] for nm in nm_list))
    dPmat = np.hstack(tuple(dP[nm[0],nm[1]] for nm in nm_list)) 

    return Pmat, dPmat, nm_list

# -----------------------------------------------------------------------------
def igrf( lon, lat, h=0, yy=2024, mm=1, dd=1, 
          Date=None, prj_str="+proj=geoc +ellps=WGS84" ):

    ## Convert input to arrays and cast to same shape:
    lon, lat, h = np.broadcast_arrays(lon, lat, h)
    shape = lon.shape

    # Flatten the arrays 
    lon = lon.flatten()
    lat = lat.flatten()
    h = h.flatten()

    ## Convert geodetic (lon,lat,h) coordinates to geocentric (phi,theta,r)
    ## Setting proj transformation object and parameters:
    ell_name = prj_str.split( '+ellps=' )[1] # ellipsoid name (e.g., 'WGS84')
    ell_param = utl.prj.get_ellps_map()[ell_name] # semimajor axes ['a'], inverse flattening ['rf']
    a_ell = ell_param['a'] # semimajor axes 
    c_ell = ( ell_param['rf'] * ell_param['a'] - ell_param['a'] ) / ell_param['rf'] # semiminor axes 
    t = utl.prj.Transformer.from_pipeline(prj_str) # transformation proj class

    # Coordinate conversion
    phi = lon
    theta = 90 - t.transform(lon, lat, h)[1]
    r = utl.ell_radius( lat, h,a=a_ell, c=c_ell )/1e3 + h/1e3 # radius in km

    # Convert degrres to radians
    lat_rad = np.radians( lat ) 
    theta_rad = np.radians( theta )
    phi_rad = np.radians( phi ) 

    # Convert intput date to datetime64 numpy array
    if Date is None :
        Date = utl.combine64( yy, mm, dd )
    _, Date = np.broadcast_arrays( lon, Date )

    # IGRF = 13th Generation International Geomagnetic Reference Field Schmidt semi-normalised spherical harmonic coefficients, degree n=1,13
    # in units nanoTesla for IGRF and definitive DGRF main-field models (degree n=1,8 nanoTesla/year for secular variation (SV))
    IGRF = np.copy( SHigrf )

    idx = IGRF[:,0] == 'g'
    IGRF[idx,0] = 1
    IGRF[~idx,0] = -1
    IGRF = IGRF.astype( float )

    RE = 6371.2 # km

    years = utl.combine64( years = IGRF[0,3:] )
    lf_nm = IGRF[1:,1:3] 
    coeff = IGRF[1:,3:]
    coeff[:,-1] = coeff[:,-1]*5 + coeff[:,-2] 

    lf_nm = lf_nm[ IGRF[1:,0] > 0 ] 

    g_coeff = coeff[ IGRF[1:,0] > 0 ]
    h_coeff = np.zeros( g_coeff.shape )
    i0 = lf_nm[:,-1] == 0
    h_coeff[ ~i0 ] = coeff[ IGRF[1:,0] < 0 ]

    dates_normalized = Date.astype('datetime64[D]')
    days_num = (dates_normalized - np.datetime64('1900-01-01')) / np.timedelta64(1, 's')
    un_days_num = np.unique( days_num )

    if utl.isiterable is False :
        Date = np.array( [Date] )

    B = np.zeros( lon.shape )
    for und in un_days_num :

        idx = days_num == und
        int_idx = np.where( idx )[0]

        date = Date[idx][0]
        r_idx, theta_idx, phi_idx = np.broadcast_arrays( r[idx], theta[idx], phi[idx] )

        shape_idx = theta[idx].shape

        # get the legendre functions
        P, dP, nm_list = ALegendreFS( theta_idx, int( lf_nm.max() ) )

        # make row vectors of wave numbers n and m:
        n, m = lf_nm[:,0], lf_nm[:,1] 
        n, m = n.reshape((1, -1)), m.reshape((1, -1))

        # Reshape them to column vectors
        r_idx = r_idx.reshape( ( -1, 1 ) )
        theta_idx = theta_idx.reshape( ( -1, 1 ) )
        phi_idx = phi_idx.reshape( ( -1, 1 ) )


        if np.any( date > years[-1] ) or np.any( date < years[0] ):
            print('Warning: You provided date(s) not covered by coefficient file \n({} to {})'.format(
                years[0], years[-1]) )

        # Interpolate and collect the coefficients at desired times
        imin = years[ years <= date ][-1] == years
        imax = years[ years >= date ][0] == years
        gci = np.zeros( ( 1, g_coeff.shape[0] ) )
        hci = np.zeros( ( 1, g_coeff.shape[0] ) )
        x = ( date-years[imin] ).astype('timedelta64[D]').astype(float)
        xp = [ 0, ( years[imax]-years[imin] ).astype('timedelta64[D]').astype(float) ]
        for i, g in enumerate( g_coeff ) :
            gp = [ float( g_coeff[i,imin] ), float( g_coeff[i,imax] ) ]
            hp = [ float( h_coeff[i,imin] ), float( h_coeff[i,imax] ) ]
            gci[0,i] = np.interp( x, xp, gp )
            hci[0,i] = np.interp( x, xp, hp )

        ## Compute cosmlon and sinmlon:
        phi_rad_idx = np.radians( phi_idx ) 
        cosmphi = np.cos( phi_rad_idx * m ) 
        sinmphi = np.sin( phi_rad_idx * m ) 

        ## Make vectors n and m that are repeated twice
        nn, mm = np.tile(n, 2), np.tile(m, 2)

        ## Calculate Br:
        Gr  = (RE / r_idx) ** (nn + 2) * (nn + 1) * np.hstack((P * cosmphi, P * sinmphi))
        Br = Gr.dot(np.hstack((gci, hci)).T).T # shape (n_times, n_coords)

        ## Calculate Btheta
        G  = -(RE / r_idx) ** (nn + 1) * np.hstack((dP * cosmphi, dP * sinmphi)) * RE / r_idx
        Btheta = G.dot( np.hstack( ( gci, hci ) ).T ).T # shape (n_times, n_coords)

        ## Calculate Bphi
        G  = -(RE / r_idx) ** (nn + 1) * mm * np.hstack((-P * sinmphi, P * cosmphi)) \
                * RE / r_idx / np.sin(np.radians(theta_idx))
        Bphi = G.dot(np.hstack((gci, hci)).T).T # shape (n_times, n_coords)

        ## Reshape and return
        outshape = tuple([Bphi.shape[0]] + list(shape_idx) )

        Br = Br.reshape(outshape) 
        Betha = Btheta.reshape(outshape)
        Bphi = Bphi.reshape(outshape) 
        lat_rad_idx = lat_rad[idx].reshape(outshape)
        theta_rad_idx = theta_rad[idx].reshape(outshape)

        ## Cartisian components of the magnetic field  
        psi = np.sin(lat_rad_idx) * np.sin(theta_rad_idx) - np.cos(lat_rad_idx) * np.cos(theta_rad_idx)
        Be = Bphi # East component of B
        Bn = -np.cos(psi) * Betha - np.sin(psi) * Br # North component of B
        Bu = -np.sin(psi) * Betha + np.cos(psi) * Br # Up component of B

        Btot = np.sqrt( Be**2 + Bn**2 + Bu**2 )
        B[int_idx] = Btot.ravel() 
        
    # Calculate inclination and declination
    I = np.arctan2(Bu, np.sqrt(Be**2 + Bn**2))
    D = np.arctan2(Be, Bn)

    # Convert from radians to degrees
    I = np.degrees(I)
    D = np.degrees(D)

    # Adjust declination to be in the range [-180, 180]
    D = (D + 180) % 360 - 180

    B = B.reshape( shape )
    # I = I.reshape( shape )
    # D = D.reshape( shape )

    # Return the total field, inclination, and declination
    return B, I, D

# -----------------------------------------------------------------------------
def emag2( lim, path=mdir + os.sep +"EMAG" + os.sep +"EMAG2_V3_20170530.npz") :

    emag = np.load( path )
    emag_data = emag['data']

    idx = utl.xy_in_lim( emag_data[:,0], emag_data[:,1], lim=lim )[2]
    emag_xyz = emag_data[ idx, : ]

    return emag_xyz

# -----------------------------------------------------------------------------
def check_lines( xyzl, prjcode_in=4326, prjcode_out=4326,
                 x_c=0, y_c=1, z_c=2, line_c=3, min_points=2,
                 order_c='same', new_xy=False ) :
    
    xyzl = np.copy( xyzl )

    if prjcode_in != prjcode_out :
        xyzl[:,x_c], xyzl[:,y_c] = utl.prjxy( prjcode_in, prjcode_out, 
                                              xyzl[:,x_c], xyzl[:,y_c] )
    
    if order_c != 'same' :
        xyzl = utl.sort_lines( xyzl, line_c=line_c, x_c=x_c, y_c=y_c, 
                               add_dist=False, order_c=order_c )

    if line_c is None :
        line_c = xyzl.shape[1]
        xyzl = np.hstack( ( xyzl, np.zeros( ( xyzl.shape[0], 1 ) ) ) )

    lines = np.unique( xyzl[ :, line_c ] )

    for l in lines :

        idx = xyzl[ :, line_c ] == l
        line = xyzl[ idx, : ]

        if line.shape[0] < min_points :

            xyzl = np.delete( xyzl, idx, axis=0 )
            print( f"Line {l} has less than {min_points} points, therefore it was deleted" )

    if new_xy is False :
        xyzl[:,x_c], xyzl[:,y_c] = utl.prjxy( prjcode_out, prjcode_in, 
                                              xyzl[:,x_c], 
                                              xyzl[:,y_c] )

    return xyzl

# -----------------------------------------------------------------------------
def average_zls_single_obs( grav_dict, max_time_gap=600, save_file=None ):
    """
    Average consecutive gravity observations for each station.
    
    Parameters:
    -----------
    grav_dict : dict
        Dictionary containing gravity observation data
    max_time_gap : float, default=600
        Maximum time gap in seconds between consecutive measurements
        to be considered part of the same group
    save_file : str, optional
        If provided, save the averaged observations to this file
        
    Returns:
    --------
    dict
        Dictionary with averaged observations
    """
    
    # Sort data by time to ensure chronological order
    time_sort_idx = np.argsort(grav_dict['TimeFloat'])
    for key in grav_dict.keys():
        if isinstance(grav_dict[key], np.ndarray):
            grav_dict[key] = grav_dict[key][time_sort_idx]

    # Find consecutive measurement groups for each station
    station_groups = []
    n_obs = len(grav_dict['Station ID'])
    
    i = 0
    while i < n_obs:
        current_station = grav_dict['Station ID'][i]
        group_indices = [i]
        
        # Look for consecutive measurements of the same station
        j = i + 1
        while j < n_obs:
            # Check if it's the same station
            if grav_dict['Station ID'][j] != current_station:
                break
            
            # Check time gap between consecutive measurements
            time_gap = grav_dict['TimeFloat'][j] - grav_dict['TimeFloat'][j-1]
            if time_gap > max_time_gap:
                break
            
            group_indices.append(j)
            j += 1
        
        # Only add groups with more than one measurement
        if len(group_indices) > 1:
            station_groups.append({
                'station': current_station,
                'indices': group_indices,
                'start_time': grav_dict['TimeFloat'][group_indices[0]],
                'end_time': grav_dict['TimeFloat'][group_indices[-1]]
            })
            print(f"Found {len(group_indices)} consecutive measurements for station {current_station}")
            print(f"  Time span: {(grav_dict['TimeFloat'][group_indices[-1]] - grav_dict['TimeFloat'][group_indices[0]])/60:.1f} minutes")
        
        # Move to the next measurement after the current group
        i = j

    if not station_groups:
        print("No consecutive measurement groups found for averaging")
        return grav_dict

    # Fields to average (numerical fields)
    numeric_fields = ['ObsG', 'Dial', 'Feedback Correction', 'Earthtide Correction', 
                    'Level Correction', 'Temperature Correction', 'Beam Error',
                    'Height', 'Elevation', 'Latitude', 'Longitude']
    
    # Fields to sum
    sum_fields = ['Elapsed Time']
    
    # Fields to take first value (string/metadata fields)
    first_value_fields = ['Observer ID', 'Serial Number', 'Date', 'Time', 
                        'Temperature Frequency']
    
    # Create masks for measurements to keep and to average
    keep_individual = np.ones(n_obs, dtype=bool)
    averaged_data = []
    
    # Process each group
    for group in station_groups:
        indices = group['indices']
        
        # Mark these indices as not to keep individually
        keep_individual[indices] = False
        
        # Extract data for this group
        group_data = {}
        for field in grav_dict.keys():
            group_data[field] = grav_dict[field][indices]
        
        # Get standard deviations for weighting
        std_devs = group_data['Standard Deviation']
        
        # Calculate weights (inverse of variance, with small epsilon to avoid division by zero)
        epsilon = 1e-10
        weights = 1.0 / (std_devs**2 + epsilon)
        
        # Normalize weights
        weights = weights / np.sum(weights)
        
        # Create averaged measurement
        averaged_measurement = {}
        
        for field in grav_dict.keys():
            field_data = group_data[field]
            
            if field == 'Station ID':
                # Use station name
                averaged_measurement[field] = group['station']
                
            elif field == 'ObsG':
                # Weighted average for ObsG (main field of interest)
                weighted_avg = np.average(field_data, weights=weights)
                averaged_measurement[field] = weighted_avg
                
            elif field in numeric_fields:
                # Weighted average for other numeric fields
                if len(field_data) > 0:
                    weighted_avg = np.average(field_data, weights=weights)
                    averaged_measurement[field] = weighted_avg
                else:
                    averaged_measurement[field] = np.nan
                    
            elif field == 'Standard Deviation':
                # Propagate uncertainty using weighted average formula
                if len(field_data) > 1:
                    # For weighted average, the uncertainty is 1/sqrt(sum(1/sigma))
                    propagated_std = 1.0 / np.sqrt(np.sum(1.0 / (std_devs**2 + epsilon)))
                    averaged_measurement[field] = propagated_std
                else:
                    averaged_measurement[field] = field_data[0]
                    
            elif field in sum_fields:
                # Sum for fields like elapsed time
                averaged_measurement[field] = np.sum(field_data)
                
            elif field in first_value_fields:
                # Take first value for metadata fields
                averaged_measurement[field] = field_data[0]
                
            elif field in ['Datetime64']:
                # Take first datetime (earliest measurement)
                averaged_measurement[field] = field_data[0]
                
            elif field in ['TimeFloat']:
                # Take first time (earliest measurement)
                averaged_measurement[field] = field_data[0]
                
            elif field in ['Day']:
                # Take first day
                averaged_measurement[field] = field_data[0]
                
            else:
                # Default: take first value
                averaged_measurement[field] = field_data[0]
        
        averaged_data.append(averaged_measurement)

    # Combine non-averaged measurements with averaged ones
    final_dict = {}
    
    # Get individual measurements that weren't averaged
    individual_indices = np.where(keep_individual)[0]
    
    # Calculate total final size
    n_final = len(individual_indices) + len(averaged_data)
    
    # Initialize final arrays
    for field in grav_dict.keys():
        
        if field in numeric_fields + ['ObsG', 'Standard Deviation'] + sum_fields + ['TimeFloat', 'Day']:
            final_dict[field] = np.zeros(n_final, dtype=float)
        
        elif field in ['Datetime64']:
            final_dict[field] = np.empty(n_final, dtype='datetime64[s]')
        
        else:
            final_dict[field] = np.empty(n_final, dtype=grav_dict[field].dtype)
    
    # Fill with individual measurements
    for i, orig_idx in enumerate(individual_indices):
        for field in grav_dict.keys():
            final_dict[field][i] = grav_dict[field][orig_idx]
    
    # Fill with averaged measurements
    for i, avg_data in enumerate(averaged_data):
        final_idx = len(individual_indices) + i
        for field in grav_dict.keys():
            final_dict[field][final_idx] = avg_data[field]
    
    # Sort final data by time
    final_time_sort_idx = np.argsort(final_dict['TimeFloat'])
    for field in final_dict.keys():
        final_dict[field] = final_dict[field][final_time_sort_idx]
    
    # Print averaging summary
    n_original = n_obs
    n_final = len(final_dict['Station ID'])
    n_averaged_groups = len(station_groups)
    total_averaged_obs = sum(len(group['indices']) for group in station_groups)
    
    print(f"\nAveraging Summary:")
    print(f"Original observations: {n_original}")
    print(f"Averaged groups: {n_averaged_groups}")
    print(f"Observations averaged: {total_averaged_obs}")
    print(f"Final observations: {n_final}")
    print(f"Reduction: {n_original - n_final} observations")
    
    # Print detailed group information
    print(f"\nDetailed group information:")
    for group in station_groups:
        indices = group['indices']
        station_std = [grav_dict['Standard Deviation'][idx] for idx in indices]
        mean_std = np.mean(station_std)
        time_span = (group['end_time'] - group['start_time']) / 60  # minutes
        print(f"  {group['station']}: {len(indices)} obs, "
              f"mean  = {mean_std:.6f} mGal, "
              f"time span = {time_span:.1f} min")
        
    if save_file is not None:
        # Save the final averaged data to a file
        _ = write_zls_obs_file( save_file, final_dict)

    return final_dict

# -----------------------------------------------------------------------------
def read_zls_obs_file( file, mode='single', tide_corr=False, average_obs=False, 
        max_time_gap=600, save_file=None ):

    # Check if the file exists
    if not os.path.isfile(file):
        raise FileNotFoundError(f"The file {file} does not exist.")

    # Load grav data from csv files
    grav_rdc_arr = np.loadtxt( file, dtype=str, delimiter=',', skiprows=1, )
    
    grav_dict = {
        'Station ID': grav_rdc_arr[:, 0],
        'Observer ID': grav_rdc_arr[:, 1],
        'Serial Number': grav_rdc_arr[:, 2],
        'Date': grav_rdc_arr[:, 3],
        'Time': grav_rdc_arr[:, 4],
        'ObsG': grav_rdc_arr[:, 5].astype(float), # Observed gravity values in mGal
        'Dial': grav_rdc_arr[:, 6].astype(float), # Dial values in mGal
        'Feedback Correction': grav_rdc_arr[:, 7].astype(float), # Feedback correction in mGal
        'Earthtide Correction': grav_rdc_arr[:, 8].astype(float), # Earth tide correction in mGal
        'Level Correction': grav_rdc_arr[:, 9].astype(float), # Level correction in mGal
        'Temperature Correction': grav_rdc_arr[:, 10].astype(float), # Temperature correction in mGal
        'Beam Error': grav_rdc_arr[:, 11].astype(float), # Beam error in mGal
        'Height': grav_rdc_arr[:, 12].astype(float), # Height in m
        'Elevation': grav_rdc_arr[:, 13].astype(float), # Elevation in m
        'Latitude': grav_rdc_arr[:, 14].astype(float), # Latitude in degrees
        'Longitude': grav_rdc_arr[:, 15].astype(float), # Longitude in degrees
        'Elapsed Time': grav_rdc_arr[:, 16].astype(float), # Elapsed time in seconds
        'Standard Deviation': grav_rdc_arr[:, 17].astype(float), # Standard deviation of the observed gravity values in mGal
        'Temperature Frequency': grav_rdc_arr[:, 18], # Temperature frequency (not used)
    }

    datetime_str_array = np.char.add( grav_dict['Date'], ' ')
    datetime_str_array = np.char.add( datetime_str_array, grav_dict['Time'] )
    datetime_str_array = np.char.replace(datetime_str_array, '/', '-')

    # Convert to numpy datetime format
    grav_dict['Datetime64'] = datetime_str_array.astype('datetime64[s]')

    # Convert to seconds 
    grav_dict['TimeFloat'] = ( grav_dict['Datetime64'] -\
        grav_dict['Datetime64'][0] ).astype('timedelta64[s]').astype(float)
    
    if mode == 'single':
        
        if tide_corr:
            # Calculate the total gravity correction
            grav_dict['ObsG'] = (grav_dict['ObsG'] - grav_dict['Earthtide Correction'])
        
        # Numering the acquisition days from 1 to N
        days = np.unique( grav_dict['Date'] )
        grav_dict['Day'] = np.zeros(len(grav_dict['Date']), dtype=int)
        for i, day in enumerate(days):
            idx = grav_dict['Date'] == day
            grav_dict['Day'][idx] = i + 1  

        # Average multiple observations for each station
        if average_obs is True:
            grav_dict = average_zls_single_obs( grav_dict, max_time_gap )

    # Create a 'Obs ID' field if it doesn't exist
    if 'Obs ID' not in grav_dict:
        grav_dict['Obs ID'] = np.arange(len(grav_dict['Station ID'])) + 1

    # Print Obs ID followed by Station ID and DateTime
    print( "\n\nObservations Summary:")
    for i in range(len(grav_dict['Obs ID'])):
        print(f"Obs ID: {grav_dict['Obs ID'][i]}, Station ID: {grav_dict['Station ID'][i]}, DateTime: {grav_dict['Datetime64'][i]}")

    # If save_file is not None, save the processed data
    if save_file is not None:

        _ = write_zls_obs_file( save_file, grav_dict )
    
    return grav_dict

# -----------------------------------------------------------------------------
def read_javad_gnss_file( file ):
    """
    Read GPS points CSV file and return as dictionary.
    Handles empty lines and other formatting issues.
    
    Parameters:
    -----------
    file : str
        Path to the GPS CSV file
    average_stations : bool, default False
        If True, averages multiple observations for each station
        Station names are identified by removing the suffix after the last underscore
        
    Returns:
    --------
    dict
        Dictionary containing GPS data with appropriate data types
    """
    
    # Check if the file exists
    if not os.path.isfile(file):
        raise FileNotFoundError(f"The file {file} does not exist.")

    # Read the file and filter out empty lines
    with open(file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # Filter out empty lines and lines with only whitespace
    filtered_lines = []
    for i, line in enumerate(lines):
        if line.strip():  # Only keep non-empty lines
            filtered_lines.append(line.strip())
    
    # Write filtered data to a temporary list for processing
    header = filtered_lines[0].split(';')
    data_lines = []
    
    for line in filtered_lines[1:]:  # Skip header
        parts = line.split(';')
        if len(parts) == len(header):  # Only keep lines with correct number of columns
            data_lines.append(parts)
    
    # Convert to numpy array
    gps_data_arr = np.array(data_lines, dtype=str)
    
    def dms_to_decimal(dms_str):
        """Convert DMS (Degrees Minutes Seconds) string to decimal degrees"""
        try:
            parts = dms_str.strip().split()
            degrees = float(parts[0])
            minutes = float(parts[1])
            seconds = float(parts[2])
            direction = parts[3]
            
            decimal = degrees + minutes/60 + seconds/3600
            if direction in ['S', 'W']:
                decimal = -decimal
            return decimal
        except (ValueError, IndexError):
            return np.nan
    
    # Convert coordinate strings to decimal degrees
    lat_decimal = np.array([dms_to_decimal(coord) for coord in gps_data_arr[:, 1]])
    lon_decimal = np.array([dms_to_decimal(coord) for coord in gps_data_arr[:, 2]])
    lat_base_decimal = np.array([dms_to_decimal(coord) for coord in gps_data_arr[:, 7]])
    lon_base_decimal = np.array([dms_to_decimal(coord) for coord in gps_data_arr[:, 8]])
    
    def safe_float_convert(arr, col_idx):
        """Safely convert string array to float, handling empty strings"""
        try:
            return np.array([float(x) if x.strip() else np.nan for x in arr[:, col_idx]])
        except (ValueError, IndexError):
            return np.full(len(arr), np.nan)
    
    def safe_int_convert(arr, col_idx):
        """Safely convert string array to int, handling empty strings"""
        try:
            return np.array([int(x) if x.strip() else 0 for x in arr[:, col_idx]])
        except (ValueError, IndexError):
            return np.full(len(arr), 0, dtype=int)
    
    gnss_dict = {
        'Name': gps_data_arr[:, 0],
        'Lat': lat_decimal,  # Latitude in decimal degrees
        'Lon': lon_decimal,  # Longitude in decimal degrees
        'Alt_m': safe_float_convert(gps_data_arr, 3),  # Altitude in meters
        'Description': gps_data_arr[:, 4],
        'Tag': gps_data_arr[:, 5],
        'CodeName': gps_data_arr[:, 6],
        'Lat_base': lat_base_decimal,  # Base latitude in decimal degrees
        'Lon_base': lon_base_decimal,  # Base longitude in decimal degrees
        'Alt_base_m': safe_float_convert(gps_data_arr, 9),  # Base altitude in meters
        'TimeUTC': gps_data_arr[:, 10],
        'Epochs': safe_int_convert(gps_data_arr, 11),
        'AntName': gps_data_arr[:, 12],
        'AntHeight_m': safe_float_convert(gps_data_arr, 13),
        'AntHeightType': gps_data_arr[:, 14],
        'HRMS_m': safe_float_convert(gps_data_arr, 15),  # Horizontal RMS in meters
        'VRMS_m': safe_float_convert(gps_data_arr, 16),  # Vertical RMS in meters
        'NGPS': safe_int_convert(gps_data_arr, 17),
        'NGLN': safe_int_convert(gps_data_arr, 18),
        'DX_m': safe_float_convert(gps_data_arr, 19),  # Delta X in meters
        'DY_m': safe_float_convert(gps_data_arr, 20),  # Delta Y in meters
        'DZ_m': safe_float_convert(gps_data_arr, 21),  # Delta Z in meters
        'Corr_XY': safe_float_convert(gps_data_arr, 22),
        'Corr_XZ': safe_float_convert(gps_data_arr, 23),
        'Corr_YZ': safe_float_convert(gps_data_arr, 24),
        'Cov_XX': safe_float_convert(gps_data_arr, 25),
        'Cov_XY': safe_float_convert(gps_data_arr, 26),
        'Cov_XZ': safe_float_convert(gps_data_arr, 27),
        'Cov_XT': safe_float_convert(gps_data_arr, 28),
        'Cov_YY': safe_float_convert(gps_data_arr, 29),
        'Cov_YZ': safe_float_convert(gps_data_arr, 30),
        'Cov_YT': safe_float_convert(gps_data_arr, 31),
        'Cov_ZZ': safe_float_convert(gps_data_arr, 32),
        'Cov_ZT': safe_float_convert(gps_data_arr, 33),
        'Cov_TT': safe_float_convert(gps_data_arr, 34),
        'RawFiles': gps_data_arr[:, 35] if gps_data_arr.shape[1] > 35 else np.array([''] * len(gps_data_arr))
    }

    # Convert time strings to numpy datetime format
    try:
        datetime_str_array = np.char.replace(gnss_dict['TimeUTC'], ' ', 'T')
        gnss_dict['Datetime64'] = datetime_str_array.astype('datetime64[s]')

        # Convert to seconds from first measurement
        gnss_dict['TimeFloat'] = (gnss_dict['Datetime64'] - 
                                gnss_dict['Datetime64'][0]).astype('timedelta64[s]').astype(float)
        
        # Extract unique survey days
        dates = np.array([str(dt)[:10] for dt in gnss_dict['Datetime64']])
        unique_dates = np.unique(dates)
        gnss_dict['Day'] = np.zeros(len(dates), dtype=int)
        for i, date in enumerate(unique_dates):
            idx = dates == date
            gnss_dict['Day'][idx] = i + 1
    except Exception as e:
        print(f"Warning: Could not process datetime data: {e}")
        gnss_dict['Datetime64'] = np.array([''] * len(gps_data_arr))
        gnss_dict['TimeFloat'] = np.zeros(len(gps_data_arr))
        gnss_dict['Day'] = np.ones(len(gps_data_arr), dtype=int)

    return gnss_dict

# ------------------------------------------------------------------
def write_zls_obs_file(file=None, grav_dict=None,
                       station_ids=None, observer_ids=None, serial_numbers=None, 
                       dates=None, times=None, obs_g=None, dials=None, 
                       feedback_corr=None, earthtide_corr=None, level_corr=None,
                       temp_corr=None, beam_error=None, heights=None, 
                       elevations=None, latitudes=None, longitudes=None, 
                       elapsed_times=None, std_devs=None, temp_freq=None):
    """
    Write gravity observation data to ZLS format CSV file.
    
    Parameters:
    -----------
    grav_dict : dict, optional
        Dictionary containing gravity data (from read_zls_obs_file).
        If provided, individual arrays are ignored.
    
    file_path : str
        Output file path for the CSV file
    
    mode : str, default 'single'
        Mode for writing ('single' or other modes if needed)
    
    Individual arrays (used only if grav_dict is None):
    [... rest of parameters ...]
        
    Returns:
    --------
    str
        Path to the written file
    """
    import os
    
    # Check input parameters
    if grav_dict is None and any(param is None for param in [
        station_ids, observer_ids, serial_numbers, dates, times, obs_g, 
        dials, feedback_corr, earthtide_corr, level_corr, temp_corr, 
        beam_error, heights, elevations, latitudes, longitudes, 
        elapsed_times, std_devs, temp_freq]):
        raise ValueError("Either provide grav_dict or all individual arrays")
    
    if file is None:
        raise ValueError("file_path must be provided")
    
    # If grav_dict is provided, extract data from it with defaults
    if grav_dict is not None:
        # Get the number of observations from a required field
        n_obs = len(grav_dict.get('Station ID', grav_dict.get('ObsG', [])))
        
        # Extract data with defaults for missing fields
        station_ids = grav_dict.get('Station ID', [f'ST_{i:03d}' for i in range(n_obs)])
        observer_ids = grav_dict.get('Observer ID', ['Unknown'] * n_obs)
        serial_numbers = grav_dict.get('Serial Number', ['Unknown'] * n_obs)
        dates = grav_dict.get('Date', ['2025/01/01'] * n_obs)
        times = grav_dict.get('Time', ['12:00:00'] * n_obs)
        obs_g = grav_dict.get('ObsG', [0.0] * n_obs)
        dials = grav_dict.get('Dial', [50000.0] * n_obs)
        feedback_corr = grav_dict.get('Feedback Correction', [0.0] * n_obs)
        earthtide_corr = grav_dict.get('Earthtide Correction', [0.0] * n_obs)
        level_corr = grav_dict.get('Level Correction', [0.0] * n_obs)
        temp_corr = grav_dict.get('Temperature Correction', [0.0] * n_obs)
        beam_error = grav_dict.get('Beam Error', [0.0] * n_obs)
        heights = grav_dict.get('Height', [0.0] * n_obs)
        elevations = grav_dict.get('Elevation', [0.0] * n_obs)
        latitudes = grav_dict.get('Latitude', [0.0] * n_obs)
        longitudes = grav_dict.get('Longitude', [0.0] * n_obs)
        elapsed_times = grav_dict.get('Elapsed Time', [0.0] * n_obs)
        std_devs = grav_dict.get('Standard Deviation', [0.001] * n_obs)
        temp_freq = grav_dict.get('Temperature Frequency', ['Unknown'] * n_obs)
    
    # Convert dates to the expected format (YYYY/MM/DD) if needed
    dates_formatted = []
    for date in dates:
        if isinstance(date, str):
            if '-' in date:
                # Convert YYYY-MM-DD to YYYY/MM/DD
                dates_formatted.append(date.replace('-', '/'))
            else:
                dates_formatted.append(date)
        else:
            # Handle numpy datetime64 or other date types
            date_str = str(date)
            if 'T' in date_str:
                date_str = date_str.split('T')[0]
            if '-' in date_str:
                date_str = date_str.replace('-', '/')
            dates_formatted.append(date_str)
    
    # Convert to numpy arrays with proper types and handle potential errors
    try:
        station_ids = np.array(station_ids, dtype=str)
        observer_ids = np.array(observer_ids, dtype=str)
        serial_numbers = np.array(serial_numbers, dtype=str)
        dates_formatted = np.array(dates_formatted, dtype=str)
        times = np.array(times, dtype=str)
        obs_g = np.array(obs_g, dtype=float)
        dials = np.array(dials, dtype=float)
        feedback_corr = np.array(feedback_corr, dtype=float)
        earthtide_corr = np.array(earthtide_corr, dtype=float)
        level_corr = np.array(level_corr, dtype=float)
        temp_corr = np.array(temp_corr, dtype=float)
        beam_error = np.array(beam_error, dtype=float)
        heights = np.array(heights, dtype=float)
        elevations = np.array(elevations, dtype=float)
        latitudes = np.array(latitudes, dtype=float)
        longitudes = np.array(longitudes, dtype=float)
        elapsed_times = np.array(elapsed_times, dtype=float)
        std_devs = np.array(std_devs, dtype=float)
        temp_freq = np.array(temp_freq, dtype=str)
    except (ValueError, TypeError) as e:
        raise ValueError(f"Error converting data to arrays: {e}")
    
    # Check all arrays have the same length
    arrays = [station_ids, observer_ids, serial_numbers, dates_formatted, times,
              obs_g, dials, feedback_corr, earthtide_corr, level_corr, temp_corr,
              beam_error, heights, elevations, latitudes, longitudes,
              elapsed_times, std_devs, temp_freq]
    
    lengths = [len(arr) for arr in arrays]
    if not all(length == lengths[0] for length in lengths):
        raise ValueError(f"All arrays must have the same length. Found lengths: {lengths}")
    
    # Define the header for the CSV file (same order as read function)
    header = [
        'Station ID',
        'Observer ID', 
        'Serial Number',
        'Date',
        'Time',
        'ObsG',
        'Dial',
        'Feedback Correction',
        'Earthtide Correction',
        'Level Correction',
        'Temperature Correction',
        'Beam Error',
        'Height',
        'Elevation',
        'Latitude',
        'Longitude',
        'Elapsed Time',
        'Standard Deviation',
        'Temperature Frequency'
    ]
    
    # Check if the output directory exists, if not create it
    output_dir = os.path.dirname(file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # Get the number of observations
    n_obs = len(station_ids)
    
    # Open file for writing
    with open(file, 'w', newline='', encoding='utf-8') as f:
        # Write header
        f.write(','.join(header) + '\n')
        
        # Write data row by row
        for i in range(n_obs):
            row_data = [
                str(station_ids[i]),
                str(observer_ids[i]),
                str(serial_numbers[i]),
                str(dates_formatted[i]),
                str(times[i]),
                f"{obs_g[i]:.5f}",
                f"{dials[i]:.5f}",
                f"{feedback_corr[i]:.5f}",
                f"{earthtide_corr[i]:.5f}",
                f"{level_corr[i]:.5f}",
                f"{temp_corr[i]:.5f}",
                f"{beam_error[i]:.5f}",
                f"{heights[i]:.3f}",
                f"{elevations[i]:.3f}",
                f"{latitudes[i]:.8f}",
                f"{longitudes[i]:.8f}",
                f"{elapsed_times[i]:.2f}",
                f"{std_devs[i]:.6f}",
                str(temp_freq[i])
            ]
            
            # Write the row
            f.write(','.join(row_data) + '\n')
    
    return file